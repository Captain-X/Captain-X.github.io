<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Collins," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="Language modeling problemWe have some finite vocabulary, which includes all of the words in our language of interest, say $\mathcal V=$ {the, a, man, telescope, Beckham, two, …}
We have an (infinite)">
<meta property="og:type" content="article">
<meta property="og:title" content="Language Modeling">
<meta property="og:url" content="https://Captain-X.github.io/nlp1-2/index.html">
<meta property="og:site_name" content="Vinci">
<meta property="og:description" content="Language modeling problemWe have some finite vocabulary, which includes all of the words in our language of interest, say $\mathcal V=$ {the, a, man, telescope, Beckham, two, …}
We have an (infinite)">
<meta property="og:updated_time" content="2018-01-23T16:20:34.091Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Language Modeling">
<meta name="twitter:description" content="Language modeling problemWe have some finite vocabulary, which includes all of the words in our language of interest, say $\mathcal V=$ {the, a, man, telescope, Beckham, two, …}
We have an (infinite)">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="https://Captain-X.github.io/nlp1-2/"/>

  <title> Language Modeling | Vinci </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Vinci</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Language Modeling
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-23T03:37:23-05:00" content="2018-01-23">
              2018-01-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="Language-modeling-problem"><a href="#Language-modeling-problem" class="headerlink" title="Language modeling problem"></a>Language modeling problem</h3><p>We have some finite vocabulary, which includes all of the words in our language of interest, say $\mathcal V=$ {the, a, man, telescope, Beckham, two, …}</p>
<p>We have an (infinite) set of strings, $\mathcal V^\dagger$. A sentence could have any sequence of words, such as<br>the STOP<br>a STOP<br>the fan STOP<br>the fan saw Beckham STOP<br>the fan saw saw STOP<br>the fan saw Beckham play for Real Madrid STOP<br>STOP</p>
<p><strong>Given a training sample of example sentences, we need to learn a probability distribution.</strong></p>
<p>$p(\text{the STOP})=10^{-12}$<br>$p(\text{the fan STOP})=10^{-8}$<br>$p(\text{the fan saw Beckham STOP})=2\times10^{-8}$<br>$p(\text{the fan saw saw STOP})=10^{-15}$<br>$p(\text{the fan saw Beckham play for Real Madrid STOP})=2\times10^{-9}$</p>
<p>Speech recognition was the original motivation. Related problems are optical character recognition, handwriting recognition. The estimation techniques developed for this problem will be VERY useful for other problems in NLP.</p>
<p>From acoustic point of view, “recognize speech” and “wreck a nice beach” are quite similar. If we simply look at a measure of how compatible this sentence is with the acoustics sentence, it’s quite possible we might confuse these two sentences. In practice, there are many other possibilities which might have a reasonable degree of fit with the acoustic input and might be quite confusable with the true sentence. If we have a language model, we can actually evaluate a probability of each of these sentences. And a language model adds some very useful information to this whole process, which is the fact that the sentence, recognize speech, is probably more probable than the sentence wreck a nice beach. In practice, modern speech recognizers use two sources of information. Firstly, they have some way of evaluating how well each of these sentences match the input from an acoustic point of view. But secondly, they also have a language model which gives a essentially prior probability over the different sentences in the language. It can be very useful in getting rid of this kind of confusions.</p>
<h4 id="A-naive-method"><a href="#A-naive-method" class="headerlink" title="A naive method"></a>A naive method</h4><p>We have $N$ training sentences. For any sentence $x_1\cdots x_n$, $c(x_1\cdots x_n)$ is the number of times the sentence is seen in our training data. A naive estimate is<br>$$<br>p(x1\cdots x_n)=\frac{c(x1\cdots x_n)}{N}<br>$$<br>But it has some very clear deficiencies. Most importantly, it will assign probability 0 to any sentence not seen in our training sample. And we know that we’re continuously seeing new sentences in a language. So, this model has no ability to generalize to new sentences. The most important question is essentially, how can we build models to improve upon this naive estimate and in particular, models which generalize well to new test sentences.</p>
<h3 id="Markov-Processes"><a href="#Markov-Processes" class="headerlink" title="Markov Processes"></a>Markov Processes</h3><p>Consider a sequence of random variables $X_1,X_2,\cdots,X_n$. Each random variable can take any value in a finite set $\mathcal V$. For now we assume the length $n$ is fixed (e.g., n = 100).</p>
<h4 id="First-Order-Markov-Process"><a href="#First-Order-Markov-Process" class="headerlink" title="First-Order Markov Process"></a>First-Order Markov Process</h4><p>$$<br>\begin{align}<br>&amp;P(X_1 = x_1,X_2 = x_2,\cdots,X_n = x_n)\\<br>=&amp;P(X_1=x_1)\prod^n_{i=2}P(X_i=x_i|X_1=x_1,\cdots,X_{i-1}=x_{i-1})\\<br>=&amp;P(X_1=x_1)\prod^n_{i=2}P(X_i=x_i|X_{i-1}=x_{i-1})<br>\end{align}\\<br>\text{The first-order Markov assumption: For any }i\in\{2,\cdots, n\}\text{, for any } x_1,\cdots,x_n,\\<br>P(X_i=x_i|X_1=x_1,\cdots,X_{i-1}=x_{i-1})=P(X_i=x_i|X_{i-1}=x_{i-1})<br>$$</p>
<h4 id="Second-Order-Markov-Process"><a href="#Second-Order-Markov-Process" class="headerlink" title="Second-Order Markov Process"></a>Second-Order Markov Process</h4><p>$$<br>\begin{align}<br>&amp;P(X_1 = x_1,X_2 = x_2,\cdots,X_n = x_n)\\<br>=&amp;P(X_1=x_1)P(X_2=x_2|X_1=x_1)\prod^n_{i=3}P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})\\<br>=&amp;\prod^n_{i=1}P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})<br>\end{align}<br>\\<br>\text{For convenience we assume } x_0=x_{-1}=\text{_, where _ is a special “start” symbol.}\\<br>\text{(assumption: }P(X_i=x_i|X_1=x_1,\cdots,X_{i-1}=x_{i-1})=P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})\text{)}<br>$$</p>
<p>We would like the length of the sequence, $n$, to be a random variable. A simple solution is to always define $X_n $= STOP, where STOP is a special symbol. The intuition is at each point, we’re generating some symbol, $X_i,$ and if we ever generate STOP, we immediately terminate the process.</p>
<h3 id="Trigram-Language-models"><a href="#Trigram-Language-models" class="headerlink" title="Trigram Language models"></a>Trigram Language models</h3><p>Each sequence of three words is called a trigram. A trigram language model consists of:</p>
<ol>
<li>a finite set $\mathcal V$</li>
<li>a set of parameters $q(w|u,v)$ for each trigram $u,v,w$ such that $w\in\mathcal V\cup\{\text{STOP}\}$, and $u,v\in\mathcal V\cup\{\text*\}$ </li>
</ol>
<p>The model treats sentences as being generated by a second-order Markov process.  For any sentence $x_1\cdots x_n$ where $x_i\in\mathcal V$ for $i = 1,\cdots,n-1$, and $x_n=$STOP, the probability of the sentence under the trigram language model is<br>$$<br>p(x_1\cdots x_n)=\prod^n_{i=1}q(x_i|x_{i-2,}x_{i-1}),\text{ where we define }x_0=x_{-1}=\text_.<br>$$<br>For the sentence $\text{the dog barks STOP}$, we would have<br>$$<br>p(\text{the dog barks STOP}) = q(\text{the}|\text{_, _})\times q(\text{dog}|\text{_,the})\times q(\text{barks}|\text{the, dog})\times q(\text{STOP}|\text{dog, barks})<br>$$<br>Now we need to estimate the parameter $q$, a natural estimate (maximum likelihood estimate) is<br>$$<br>q(w_i|w_{i-2},w_{i-1})=\frac{\text{Count}(w_{i-2},w_{i-1},w_i)}{\text{Count}(w_{i-2},w_{i-1})}<br>$$<br><strong>Sparse Data Problem</strong>: We have $N^3$ parameters in the model. If $N=20,000$, there would be $8\times10^{12}$ parameters. This is a very large number, in comparison to the number of training examples that we have. In many cases, the counts $\text{Count}(w_{i-2},w_{i-1},w_i)$ on the numerator may be equal to 0. Because we simply haven’t seen this particular triagram in training data, in which case the estimate will be equal to 0. That’s problematic because there are so many trigrams possible. Seeing the trigram zero times in training doesn’t mean that we should say the estimate is equal to 0. Still, in some cases, the denominator $\text{Count}(w_{i-2},w_{i-1})$ may be zero, in which case the ratio is completely undefined and the estimate really falls apart.</p>
<h3 id="Perplexity"><a href="#Perplexity" class="headerlink" title="Perplexity"></a>Perplexity</h3><p>We have some test data, $m$ sentences $s_1,s_2,s_3,\cdots,s_m$. We could look at the probability under our model $\prod_{i=1}^m p(s_i)$. Or more conveniently, the log probability<br>$$<br>\log\prod_{i=1}^m p(s_i)=\sum_{i=1}^m\log p(s_i）<br>$$<br>The higher the log probability, the better our language model is. In fact the usual evaluation measure is perplexity.<br>$$<br>\text{Perplexity}=2^{-l}\qquad\text{where}\qquad l=\frac{1}{M}\sum^m_{i=1}\log p(s_i)\\<br>\text{and M is the total number of words in the test data.}<br>$$<br>Perplexity is a measure of effective “branching factor”. If we assume $q(w|u,v)=1/N$, then $l=\log (1/N)$, and $\text{Perplexity}=N$. It describes the extent of how many different branches each point in the sentence can have in average. Under good models, the “next word” should be well determined and thus there are less branches.</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Collins/" rel="tag">#Collins</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/nlp1-1/" rel="next" title="Introduction to NLP">
                <i class="fa fa-chevron-left"></i> Introduction to NLP
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/nlp1-3/" rel="prev" title="Parameter Estimation in Language Models">
                Parameter Estimation in Language Models <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://zccz14.com/images/avatar.png"
               alt="dada" />
          <p class="site-author-name" itemprop="name">dada</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">13</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Captain-X/" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://zccz14.com/" title="zccz14" target="_blank">zccz14</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Language-modeling-problem"><span class="nav-number">1.</span> <span class="nav-text">Language modeling problem</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#A-naive-method"><span class="nav-number">1.1.</span> <span class="nav-text">A naive method</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Markov-Processes"><span class="nav-number">2.</span> <span class="nav-text">Markov Processes</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#First-Order-Markov-Process"><span class="nav-number">2.1.</span> <span class="nav-text">First-Order Markov Process</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Second-Order-Markov-Process"><span class="nav-number">2.2.</span> <span class="nav-text">Second-Order Markov Process</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Trigram-Language-models"><span class="nav-number">3.</span> <span class="nav-text">Trigram Language models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Perplexity"><span class="nav-number">4.</span> <span class="nav-text">Perplexity</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">dada</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
