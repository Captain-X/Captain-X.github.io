<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="Vinci">
<meta property="og:url" content="https://Captain-X.github.io/index.html">
<meta property="og:site_name" content="Vinci">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Vinci">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="https://Captain-X.github.io/"/>

  <title> Vinci </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Vinci</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp9-2/" itemprop="url">
                  Global Linear Models
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-02-03T11:07:33-05:00" content="2018-02-03">
              2018-02-03
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp9-1/" itemprop="url">
                  Unsupervised-Learning Brown Cluster
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-02-03T11:07:31-05:00" content="2018-02-03">
              2018-02-03
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp8-2/" itemprop="url">
                  Log-Linear Models for History-based Parsing
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-02-02T04:36:57-05:00" content="2018-02-02">
              2018-02-02
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp8-1/" itemprop="url">
                  Log-Linear Models for Tagging(MEMMs)
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-02-02T04:36:56-05:00" content="2018-02-02">
              2018-02-02
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp7-1/" itemprop="url">
                  Log-Linear Models
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-30T09:47:09-05:00" content="2018-01-30">
              2018-01-30
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><h5 id="Example-1-Trigram-Model-in-Language-Modeling-Problem"><a href="#Example-1-Trigram-Model-in-Language-Modeling-Problem" class="headerlink" title="Example 1: Trigram Model in Language Modeling Problem"></a>Example 1: Trigram Model in Language Modeling Problem</h5><p>Say we have a document, the first $i-1$ words $w_1\cdots w_{i-1}=$ <i>Third, the notion “grammatical in English” cannot be identified in any way with the notion “high order of statistical approximation to English”. It is fair to assume that neither sentence (1) nor (2) (nor indeed any part of these sentences) has ever occurred in an English discourse. Hence, in any statistical</i>. We now need to estimate a distribution $p(w_i|w_1\cdots w_{i-1})$. Under trigram model, the estimate is<br>$$<br>\begin{align}<br>q(\text{model}|w_1\cdots w_{i-1})=&amp;\lambda_1 q_{ML}(\text{model}|w_{i-2}=\text{any},w_{i-1}=\text{statistical})+\ <br>&amp;\lambda_2 q_{ML}(\text{model}|w_{i-1}=\text{statistical})+\\<br>&amp;\lambda_3q_{ML}(\text{model})<br>\end{align}\\<br>\text{where }\sum_i \lambda_i=1, \text{and }\lambda_i\ge0,q_{ML}(y|x)=\frac{\text{Count}(x,y)}{\text{Count}(x)}<br>$$<br>However, this estimate makes use of only bigram, trigram and unigram estimates, and fails to use many other useful features of $w_i\cdots w_{i-1}$, e.g.,<br>$q_{ML}(\text{model}|w_{i-2}=\text{any})$<br>$q_{ML}(\text{model}|w_{i-1}\text{ is an adjective})$<br>$q_{ML}(\text{model}|w_{i-1} \text{ ends in “ical”})$<br>$q_{ML}(\text{model}|\text{author}=\text{Chomsky})$<br>$q_{ML}(\text{model}|\text{“model” does not occur somewhere in }w_1\cdots w_{i-1})$<br>$q_{ML}(\text{model}|\text{“grammatical” occurs somewhere in }w_1\cdots w_{i-1})$<br>A naive approach is to use the weighted sum of all these features as the estimate, but this quickly becomes very unwieldy.</p>
<h5 id="Example-2-Hidden-Markov-Model-in-Tagging-Problem"><a href="#Example-2-Hidden-Markov-Model-in-Tagging-Problem" class="headerlink" title="Example 2: Hidden Markov Model in Tagging Problem"></a>Example 2: Hidden Markov Model in Tagging Problem</h5><h3 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h3><p>$\mathcal X$</p>
<h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><h3 id="Parameter-Estimation"><a href="#Parameter-Estimation" class="headerlink" title="Parameter Estimation"></a>Parameter Estimation</h3><h3 id="Smoothing-Regularization"><a href="#Smoothing-Regularization" class="headerlink" title="Smoothing-Regularization"></a>Smoothing-Regularization</h3>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp4-1/" itemprop="url">
                  Lexicalized PCFGs
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-29T23:19:45-05:00" content="2018-01-29">
              2018-01-29
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Lexicalization-of-a-treebank"><a href="#Lexicalization-of-a-treebank" class="headerlink" title="Lexicalization of a treebank"></a>Lexicalization of a treebank</h3><p>The first key idea is to add annotations specifying the <strong>head</strong> of each rule, which in some sence is an additional piece of information in our CFG. Each context-free rule has one “special” child that is the head of the rule. The intuition is that the head is the central sub-constituent and the semantic predicate of each rule. e.g., <br><img src="../img/nlp/heads.png"></p>
<p><strong>Rules which Recover Heads: An Example for NPs</strong><br>If the rule contains NN(singular noun), NNS(plural noun), or NNP(proper noun): Choose the rightmost NN, NNS, or NNP<br>Else If the rule contains an NP: Choose the leftmost NP<br>Else If the rule contains a JJ: Choose the rightmost JJ<br>Else If the rule contains a CD(number): Choose the rightmost CD<br>Else Choose the rightmost child<br>e.g.,<br>NP $\Rightarrow$  DT    NNP    <strong>NN</strong><br>NP $\Rightarrow$  DT    NN    <strong>NNP</strong><br>NP $\Rightarrow$  <strong>NP</strong>    PP<br>NP $\Rightarrow$  DT    <strong>JJ</strong><br>NP $\Rightarrow$  <strong>DT</strong></p>
<p><strong>Rules which Recover Heads: An Example for VPs</strong><br>If the rule contains Vi or Vt: Choose the leftmost Vi or Vt<br>Else If the rule contains an VP: Choose the leftmost VP<br>Else Choose the leftmost child<br>e.g.,<br>VP $\Rightarrow$ <strong>Vt</strong> NP<br>VP $\Rightarrow$ <strong>VP</strong> PP</p>
<p>Then we can transform our tree banks by adding head item for each non-terminal. Then the size of the set of non-terminals increases to $|S|\times|V|$.<img src="../img/nlp/adding_headwords.png"></p>
<h3 id="Lexicalized-probabilistic-context-free-grammars"><a href="#Lexicalized-probabilistic-context-free-grammars" class="headerlink" title="Lexicalized probabilistic context-free grammars"></a>Lexicalized probabilistic context-free grammars</h3><h4 id="Context-Free-Grammars-in-Chomsky-Normal-Form"><a href="#Context-Free-Grammars-in-Chomsky-Normal-Form" class="headerlink" title="Context-Free Grammars in Chomsky Normal Form"></a>Context-Free Grammars in Chomsky Normal Form</h4><p>A context free grammar $G = (N,\Sigma,R,S)$ in Chomsky Normal Form is as follows:<br>$\qquad N$ is a set of non-terminal symbols<br>$\qquad\Sigma$ is a set of terminal symbols<br>$\qquad R$ is a set of rules which take one of two forms:<br>$\qquad\qquad X \rightarrow Y_1Y_2$ for $X\in N$, and $Y_1,Y_2\in N$<br>$\qquad\qquad X \rightarrow Y $ for $X \in N$, and $Y\in\Sigma$ <br>$\qquad S\in N$ is a distinguished start symbol<br>We can find the highest scoring parse under a PCFG in this form, in $O(n^3|N|^3)$ time where $n$ is the length of the string being parsed.</p>
<h4 id="Lexicalized-Context-Free-Grammars-in-Chomsky-Normal-Form"><a href="#Lexicalized-Context-Free-Grammars-in-Chomsky-Normal-Form" class="headerlink" title="Lexicalized Context-Free Grammars in Chomsky Normal Form"></a>Lexicalized Context-Free Grammars in Chomsky Normal Form</h4><p>$\qquad N$ is a set of non-terminal symbols<br>$\qquad\Sigma$ is a set of terminal symbols<br>$\qquad R$ is a set of rules which take one of three forms:<br>$\qquad\qquad X(h) \rightarrow_1 Y_1(h)Y_2(w)$ for $X\in N$, and $Y_1,Y_2\in N$, and $h,w\in\Sigma$<br>$\qquad\qquad X(h) \rightarrow_2 Y_1(w)Y_2(h)$ for $X\in N$, and $Y_1,Y_2\in N$, and $h,w\in\Sigma$<br>$\qquad\qquad X \rightarrow h $ for $X \in N$, and $h\in\Sigma$ <br>$\qquad S\in N$ is a distinguished start symbol<br></p>
<h4 id="Parsing-with-Lexicalized-CFGs"><a href="#Parsing-with-Lexicalized-CFGs" class="headerlink" title="Parsing with Lexicalized CFGs"></a>Parsing with Lexicalized CFGs</h4><p>The new form of grammar looks just like a Chomsky normal form CFG, but with potentially $O(|\Sigma|^2|N|^3)$ possible rules. Naively, parsing an $n$ word sentence using the dynamic programming algorithm will take $O(n^3|\Sigma|^2|N|^3)$ time. But $|\Sigma|$ can be huge. A crucial observation is that at most $O(n^2 |N|^3)$ rules can be<br>applicable to a given sentence $w_1w2\cdots w_n$ of length $n$. This is because any rules which contain a lexical item that is not one of $w_1\cdots w_n$, can be safely discarded. So we can parse in $O(n^5|N|^3)$ time.</p>
<h3 id="Parameter-estimation"><a href="#Parameter-estimation" class="headerlink" title="Parameter estimation"></a>Parameter estimation</h3><p><img src="../img/nlp/lexicalized_tree.png"></p>
<p>p(t) = q(S(saw)$\rightarrow_2$ NP(man) VP(saw))$\times$q(NP(man)$\rightarrow_2$ DT(the) NN(man))$\times$q(VP(saw) $\rightarrow_1$ VP(saw) PP(with))$\times$q(VP(saw) $\rightarrow_1$ Vt(saw) NP(dog))$\times$q(PP(with)$\rightarrow_1$ IN(with) NP(telescope))$\times\cdots$, from which we can see</p>
<p>These rules incorporate rich sources of lexical information. We now have parameters that explicitly model dependencies between lexical items. For example, q(VP(saw) $\rightarrow_1$ VP(saw) PP(with)) gives how likely is for a prepositional phrase with prepositon “with” to modify verb phrase with head “see”. The new parameters now have direct access to important lexical information. However, the challenge is that we have very large number of rules and relatively limited number of training data.</p>
<h4 id="A-Model-from-Charniak-1997"><a href="#A-Model-from-Charniak-1997" class="headerlink" title="A Model from Charniak (1997)"></a>A Model from Charniak (1997)</h4><p>An example parameter in a Lexicalized PCFG:$q(S(\text{saw}) \rightarrow_2 NP(\text{man})\quad VP(\text{saw}))$<br>First step: decompose this parameter into a product of two parameters<br>$$<br>q(S(\text{saw}) \rightarrow_2 NP(\text{man})\quad VP(\text{saw}))<br>= q(S \rightarrow_2NP\quad VP|S, \text{saw}) \times q(\text{man}|S \rightarrow_2 NP\quad VP, \text{saw})<br>$$<br>Second step: use smoothed estimation for the two parameter estimates<br>$$<br>q(S \rightarrow_2NP\;\; VP|S, \text{saw})=\lambda_1\times q_{ML}(S \rightarrow_2NP\;\;VP|S, \text{saw})+\lambda_2\times q_{ML}(S \rightarrow_2NP\;\; VP|S)\\<br>q(\text{man}|S \rightarrow_2 NP\;\; VP, \text{saw})=\lambda_3\times q_{ML}(\text{man}|S \rightarrow_2 NP\;\; VP, \text{saw})+\lambda_4 \times q_{ML}(\text{man}|S \rightarrow_2 NP\;\; VP)+\lambda_5 \times q_{ML}(\text{man}|NP)<br>$$</p>
<h4 id="Other-important-details"><a href="#Other-important-details" class="headerlink" title="Other important details"></a>Other important details</h4><p>Need to deal with rules with more than two children, e.g.,<br>VP(told) $\rightarrow$ V(told)    NP(him)    PP(on)    SBAR(that)<br>(add intermediate rules)</p>
<p>Need to incorporate parts of speech (useful in smoothing)<br>VP-V(told) $\rightarrow$ V(told)    NP-PRP(him)    PP-IN(on)    SBAR-COMP(that)<br>(It gives us more information in this lexicalize rule)</p>
<p>Need to encode preferences for close attachment</p>
<p>Further reading: Michael Collins. 2003. Head-Driven Statistical Models for Natural Language Parsing. In Computational Linguistics.</p>
<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><h4 id="Constituent-accuracies"><a href="#Constituent-accuracies" class="headerlink" title="Constituent accuracies"></a>Constituent accuracies</h4><p>We can map our parse tree to a set of constituents, both for human-analyated tree, which act as the gold standard in test data, and for output parse tree of our parser.</p>
<p><img src="../img/nlp/precision_recall.png"></p>
<p>$$<br>\text{Recall}=100\%\times\frac{C}{G} \qquad\text{Precision}=100\%\times\frac{C}{P}<br>$$</p>
<h4 id="Dependency-Accuracies"><a href="#Dependency-Accuracies" class="headerlink" title="Dependency  Accuracies"></a>Dependency  Accuracies</h4><p>We can also convert a tree into a set of dependencies. An example is as follows, where the first column is the headword and the second is the modifier word. The rule production is a label of the type of dependency involved. For example, $&lt;\text{saw}_3, \text{man}_2,S\rightarrow_2 NP\text{ }\text{ }VP&gt;$ reveals a subject-verb dependency. Note that the number of dependencies is equal to the number of words in the sentence. We can then calculate recall and precision. The general recall and precision should be the same since the tree in gold standard and the tree testing will have the same number of dependencies. But we can calculate precision/recall on particular dependency types (e.g., look at all subject/verb dependencies$\Rightarrow $all dependencies with label S$\rightarrow_2$ NP VP) and identify those problematic dependencies.<br><img src="../img/nlp/dependency.png"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp3-2/" itemprop="url">
                  Probabilistic Context-Free Grammars
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-25T09:43:15-05:00" content="2018-01-25">
              2018-01-25
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>###Probabilistic Context-Free Grammars (PCFGs)</p>
<p>Under PCFG, each rule is assigned with a probability.<br><img src="../img/nlp/pcfg.png"></p>
<p>Probability of a tree $t$ with rules $\alpha_1\rightarrow\beta_1,\alpha_2\rightarrow\beta_2,\cdots, \alpha_n\rightarrow\beta_n$ is $p(t)=\prod_{i=1}^n q(\alpha_1\rightarrow\beta_1)$. A PCFG assigns a probability to each left-most derivation, or parse-tree, allowed by the underlying CFG. Say we have a sentence $s$, set of derivations for that sentence is $\mathcal T(s)$. Then a PCFG assigns a probability $p(t)$ to each member of $\mathcal T(s)$. i.e., we now have a ranking in order of probability. The most likely parse tree for a sentence $s$ is $\arg \max_{t\in \mathcal T(s)} p(t)$.</p>
<p><strong>Deriving a PCFG from a Treebank</strong><br>Given a set of example trees (a treebank), the underlying CFG can simply be all rules seen in the corpus<br>Maximum Likelihood estimates for $q_(\alpha\rightarrow\beta)$:<br>$$q_{ML}(\alpha\rightarrow\beta) = \frac{\text{Count}(\alpha\rightarrow\beta)}{\text{Count}(\alpha)}$$<br>where the counts are taken from a training set of example trees.<br>If the training data is generated by a PCFG, then as the training data size goes to infinity, the maximum-likelihood PCFG will converge to the same distribution as the”true” PCFG.</p>
<h3 id="The-CKY-Algorithm"><a href="#The-CKY-Algorithm" class="headerlink" title="The CKY Algorithm"></a>The CKY Algorithm</h3><h4 id="Chomsky-Normal-Form"><a href="#Chomsky-Normal-Form" class="headerlink" title="Chomsky Normal Form"></a>Chomsky Normal Form</h4><p>A context free grammar $G = (N,\Sigma,R,S)$ in Chomsky Normal Form is as follows:<br>$\qquad N$ is a set of non-terminal symbols<br>$\qquad\Sigma$ is a set of terminal symbols<br>$\qquad R$ is a set of rules which take one of two forms:<br>$\qquad\qquad X \rightarrow Y_1Y_2$ for $X\in N$, and $Y_1,Y_2\in N$<br>$\qquad\qquad X \rightarrow Y $ for $X \in N$, and $Y\in\Sigma$ <br>$\qquad S\in N$ is a distinguished start symbol<br></p>
<p>It seems that the form has put many restrictions to the grammar, but in fact any PCFG can be converted into a CFG in Chomsky Normal Form by some tricks. e.g., $\{\text{VP}\rightarrow \text{Vt}\quad \text{NP}\quad \text{PP}\} (0.2)$ can be converted to $\{\text{VP}\rightarrow \text{Vt-NP}\quad \text{PP}\} (0.2)$ and $\{\text{Vt-NP}\rightarrow \text{Vt}\quad \text{NP}\} (1.0)$</p>
<p>A Dynamic Programming Algorithm<br>Given a PCFG in Chomsky Normal Form and a sentence $s$, how do we find $\arg\max_{t\in\mathcal T(s) }p(t)$<br>Notation:<br>$n$ = number of words in the sentence<br>$w_i$= i’th word in the sentence<br>$N$ = the set of non-terminals in the grammar<br>$S$ = the start symbol in the grammar<br>Define a dynamic programming table<br>$\pi[i,j,X]$ = maximum probability of a constituent with non-terminal X spanning words $i,\cdots,j$ inclusive.<br>Our goal is to calculate $\max_{t\in\mathcal T(s) }p(t)=\pi[1,n,S]$.</p>
<p>We are searching through all different decompositions, that is, search for all different choices of rules and all different choices of splitting point, and will thereby find the single best way of reaching $X$ spanning $w_i$ to $w_j$</p>
<p><strong>Input:</strong> a sentence $s$ = $x_1\cdots x_n$, a PCFG $G = (N,\Sigma,S,R,q)$.<br><strong>Initialization:</strong> For all $i\in\{1,\cdots,n\}$, for all $X\in N$, $\pi(i,i,X)=q(X\rightarrow x_i)$<br><strong>Algorithm:</strong><br>For $l =1,\cdots, (n-1)$<br>For $i = 1,\cdots,(n-l)$<br>Set $j = i + l$<br>For all $X \in N$, calculate<br>$$<br>\pi(i,j,X)=\underset{s\in\{i,\cdots,(j-1)\}}{\underset{(X\rightarrow YZ)\in R}{\max}} (q(X\rightarrow YZ)\times\pi(i,s,Y)\times\pi(s+1,j,Z))\\<br>bp(i,j,X)=\underset{s\in\{i,\cdots,(j-1)\}}{\underset{(X\rightarrow YZ)\in R}{\arg\max}} (q(X\rightarrow YZ)\times\pi(i,s,Y)\times\pi(s+1,j,Z))\\<br>$$</p>
<h3 id="Weaknesses-of-PCFGs"><a href="#Weaknesses-of-PCFGs" class="headerlink" title="Weaknesses of PCFGs"></a>Weaknesses of PCFGs</h3><h4 id="Lack-of-sensitivity-to-lexical-information"><a href="#Lack-of-sensitivity-to-lexical-information" class="headerlink" title="Lack of sensitivity to lexical information"></a>Lack of sensitivity to lexical information</h4><p>Example 1</p>
<p><img src="../img/nlp/weakness_eg1.png"><br>Consider a word in the tree, once we condition on the non-terminal above that word, i.e., part of speech, the assumption in PCFG is that the choice of this word is conditionally independent of everything else in the tree, which means we are making an assumption that the part of speech carries all the information we could possibly need about the identity of this word. It is a very strong and bad assumption</p>
<p>Example 2 (PP Attachment Ambiguity)</p>
<p><img src="../img/nlp/weakness_eg2.png"><img src="../img/nlp/weakness_eg2a.png"><br>The parsing of every sentence in this syntactic structure (specifically the attachment of PP), depends only on between $q(VP\rightarrow VP\quad PP)$ and $q(NP\rightarrow NP\quad PP)$ which is greater. This decision is very sub-optimal because it is completely based on the prior of the parse trees and ignores the lexical information lied in these phrases. It is unable to distinguish the two different attachments, classifying them into the same parse tree if two probabilities are already obtained. <strong>Attachment decision is completely independent of the words.</strong></p>
<p>Example 3 (Coordination Ambiguity)</p>
<p><img src="../img/nlp/weakness_eg3.png"><img src="../img/nlp/weakness_eg3a.png"><br>Here the two parses have identical rules, and therefore have identical probability under any assignment of PCFG rule probabilities</p>
<h4 id="Lack-of-sensitivity-to-structural-frequencies"><a href="#Lack-of-sensitivity-to-structural-frequencies" class="headerlink" title="Lack of sensitivity to structural frequencies"></a>Lack of sensitivity to structural frequencies</h4><p>Example: president of a company in Africa (PP attachment for noun)<br><img src="../img/nlp/weakness_eg4.png"><br>Both parses have the same rules, therefore receive same probability under a PCFG. “Close attachment” (structure (a)) is twice as likely in Wall Street Journal text. An even more pronounced of close attachment preference is attachment for verbs.e.g, <i>John was believed to have been shot by Bill</i>. Close attachment preference in case of verbs are approximately 20 times more likely.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp3-1/" itemprop="url">
                  Parsing and Context-Free Grammars
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-24T04:18:14-05:00" content="2018-01-24">
              2018-01-24
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>###The parsing problem</p>
<p>A parse tree is a hierarchical decomposition of a sentence, encoding the grammatical structure within the sentence.</p>
<p><strong>The information conveyed by parse trees</strong></p>
<p>(1) Part of speech for each word (N = noun, V = verb, DT = determiner)<br><img src="../img/nlp/parse_info.png"></p>
<p>(2) Phrases<br>Noun Phrases (NP): “the burglar”, “the apartment”<br>Verb Phrases (VP): “robbed the apartment”<br>Sentences (S): “the burglar robbed the apartment”</p>
<p>(3) Useful Relationships<br><img src="../img/nlp/parse_relation.png"><br>This template tells us that “the burglar” is the subject of “robbed”</p>
<p>###Context-free grammars</p>
<p>A context free grammar $G = (N,\Sigma,R,S)$ where:<br>$\qquad N$ is a set of non-terminal symbols<br>$\qquad\Sigma$ is a set of terminal symbols<br>$\qquad R$ is a set of rules of the form $X\rightarrow Y_1Y_2\cdots Y_n$ for $n\ge0,X\in N,Y_i\in(N\cup \Sigma)$<br>$\qquad S\in N$ is a distinguished start symbol</p>
<p><strong>An example</strong><br><img src="../img/nlp/cfg.png"></p>
<p>####Left-most deriviation</p>
<p>A left-most derivation is a sequence of strings $s_1\cdots s_n$, where $s_1=S$, the start symbol, $s_n\in\Sigma\text{*}$, i.e., $s_n$ is made up of terminal symbols only. Each $s_i$ for $i=2\cdots n $ is derived from $s_{i-1}$ by picking the left-most non-terminal $X$ in $s_{i-1}$ and replacing it by some $\beta$ where $X\rightarrow\beta$ is a rule in $R$. For example: [S], [NP VP], [D N VP], [the N VP], [the man VP], [the man Vi], [the man sleeps].</p>
<p>A CFG defines a set of possible derivations. (In most cases the set is infinite) A string $s\in\Sigma\text{*}$ is in the language defined by the CFG if there is at least one derivation that yields $s$. Each string in the language generated by the CFG may have more than one derivation (“ambiguity”). For example, the sentence “he drove down the street in the car” can be derived from:<br>[S], [NP VP], [he VP], <strong>[he VP PP], [he VB PP PP], [he drove PP PP], [he drove IN NP PP]</strong>, [he drove down NP PP], [he drove down DT NN PP], [he drove down the NN PP], [he drove down the street PP], [he drove down the street IN NP], [he drove down the street in NP], [he drove down the street in DT NN], [he drove down the street in the NN], [he drove down the street in the car]<br>[S], [NP VP], [he VP], <strong>[he VB PP], [he drove PP], [he drove IN NP], [he drove down NP]</strong>, [he drove down NP PP], [he drove down DT NN PP], [he drove down the NN PP], [he drove down the street PP], [he drove down the street IN NP], [he drove down the street in NP], [he drove down the street in DT NN], [he drove down the street in the NN], [he drove down the street in the car]</p>
<p>We use parse tree to represent these deriviations. Different parse trees correspond to difference interpretations of the underlying sentence. In the first parse tree, the PP “in the car” is modifying the entire verb phrase “drove down the street” (by far the most likely interpretation), while in the second tree, it is modifying the noun phrase “the street” (which means the street is in the car).<br><img src="../img/nlp/deriviation2.png"><img src="../img/nlp/deriviation1.png"></p>
<p>The problem with parsing—ambiguity: there are several possible syntactic structures for the sentence corresponding to various different interpretations, most of which are quite unlikely but all of which are nevertheless sytactically well formed.</p>
<p>###A brief sketch of the syntax of English</p>
<p>####Noun phrases</p>
<p>(NN = singular noun, NNS = plural noun, NNP = proper noun, DT = determiner, JJ = adjective)<br>A fragment of noun phrase grammar<br><img src="../img/nlp/noun_phrase1.png"><br><img src="../img/nlp/noun_phrase2.png"></p>
<p>####Prepositional phrases</p>
<p>An extended grammar<br>(IN = preposition)<br><img src="../img/nlp/pp1.png"><br><img src="../img/nlp/pp2.png"></p>
<h4 id="Verbs-verb-phrases-and-sentences"><a href="#Verbs-verb-phrases-and-sentences" class="headerlink" title="Verbs, verb phrases, and sentences"></a>Verbs, verb phrases, and sentences</h4><p>Basic Verb Types<br>$\qquad$Vi = Intransitive verb<br>$\qquad$Vt = Transitive verb<br>$\qquad$Vd = Ditransitive verb (give)<br>Basic VP Rules<br>$\qquad$VP $\rightarrow$ Vi<br>$\qquad$VP $\rightarrow$ Vt NP<br>$\qquad$VP $\rightarrow$ Vd NP NP (give the dog a ball)<br>Basic S Rule<br>$\qquad$S  $\rightarrow$ NP VP</p>
<p>PPs Modifying Verb Phrases<br>$\qquad$VP $\rightarrow$ VP PP (gave the dog a ball on Wednesday)</p>
<p>####Complementizers, and SBARs</p>
<p>COMP = complementizer (e.g., that)<br>SBAR $\rightarrow$ COMP S</p>
<p>New Verb Types<br>$\qquad$V[5] (e.g., said, reported)<br>$\qquad$V[6] (e.g., told, informed)<br>$\qquad$V[7] (e.g., bet)<br>New VP Rules<br>$\qquad$VP  $\rightarrow$  V[5] SBAR  (said that the man sleeps)<br>$\qquad$VP  $\rightarrow$  V[6] NP SBAR  (told the dog that the mechanic likes the pigeon)<br>$\qquad$VP  $\rightarrow$  V[7] NP NP SBAR  (bet the pigeon $50 that the mechanic owns a fast car)</p>
<p>####Coordination</p>
<p>A New Part-of-Speech:<br>$\qquad$CC = Coordinator (e.g., and, or, but)<br>New Rules<br>$\qquad$NP $\rightarrow$ NP CC NP<br>$\qquad$$\bar N$ $\rightarrow$ $\bar N$ CC $\bar N$<br>$\qquad$VP $\rightarrow$ VP CC VP<br>$\qquad$S $\rightarrow$ S CC S<br>$\qquad$SBAR $\rightarrow$ SBAR CC SBAR</p>
<h4 id="Only-a-scratch-of-the-surface"><a href="#Only-a-scratch-of-the-surface" class="headerlink" title="Only a scratch of the surface!"></a>Only a scratch of the surface!</h4><p><strong>Agreement</strong>: The dogs laugh v.s. The dog laughs<br><strong>Wh-movement</strong>: The dog that the cat liked … (The direct object of like is the dog instead of …)<br><strong>Active v.s. passive</strong>: The dog saw the cat v.s. The cat was seen by the dog</p>
<p>###Examples of ambiguous structures</p>
<p>####Part-of-Speech ambiguity</p>
<p>The word “duck” can serve as both NN and Vi. This kind of ambiguity frequently leads to multiple possible parse structures for a particular sentence.<br><img src="../img/nlp/part_ambiguity.png"></p>
<p>####Prepositional phrase attachment</p>
<p><img src="../img/nlp/deriviation2.png"><img src="../img/nlp/deriviation1.png"></p>
<p>Another example: John was believed to have been shot by Bill, which can be interpreted as Bill shot John or Bill believe John has been shot. But the former is more plausible due to a strong preference for PP to modify the most recent verb.</p>
<p>####Noun Premodifier</p>
<p><img src="../img/nlp/noun_premodifier.png"><br>The JJ “fast” can modify “car” or “mechanic”. </p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp2-2/" itemprop="url">
                  Hidden Markov Model Taggers
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-23T09:34:04-05:00" content="2018-01-23">
              2018-01-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Basic-definitions"><a href="#Basic-definitions" class="headerlink" title="Basic definitions"></a>Basic definitions</h3><p>We have an input sentence $x=x_1,x_2,\cdots,x_n$, and a tag sequence $y=y_1,y_2,\cdots,y_n$. We use an Hidden Markov Model to define a joint probability $p(x_1,x_2,\cdots,x_n,y_1,y_2,\cdots,y_n)$ for any sentence $x_1,x_2,\cdots,x_n$ and tag sequence $y_1,y_2,\cdots,y_n$ of the same length. Then the most likely tag sequence for $x$ is $\operatorname{argmax}_{y_1\cdots y_n}p(x_1\cdots x_n,y_1\cdots y_n)$</p>
<h4 id="Trigram-Hidden-Markov-Models"><a href="#Trigram-Hidden-Markov-Models" class="headerlink" title="Trigram Hidden Markov Models"></a>Trigram Hidden Markov Models</h4><p>We define $\mathcal V$ is the set of all possible vocabulary, and $\mathcal S$ is the set of all possible tags ($|\mathcal S|\approx50$). For any sentence $x_1\cdots x_n$ where $x_i\in\mathcal V$ , and any tag sequence $y_1\cdots y_{n+1}$ where $y_i\in \mathcal S$ and $y_{n+1}=\text{STOP}$, the joint probability of the sentence and tag sequence is<br>$$<br>p(x_1\cdots x_n,y_1\cdots y_{n+1})=\prod_{i=1}^{n+1}q(y_i|y_{i-2},y_{i-1})\prod_{i=1}^ne(x_i|y_i)\\<br>$$<br>Parameters of the model:<br>$q(s|u,v)$ for any $s\in\mathcal S\cup\{\text{STOP}\},u,v\in\mathcal S\cup\{*\}$ (trigram parameter)<br>$e(x|s) $ for any $s\in\mathcal S,x\in\mathcal V$ (emission parameter)</p>
<p><strong>An example</strong><br>If we have $n=3$, $x_1\cdots x_3$ equal to the sentence <i>the dog laughs</i>, and $y_1\cdots y_4$ equal to the tag sequence $\text{D N V STOP}$, then<br>$$<br>p(x_1\cdots x_n,y_1\cdots y_{n+1})=q(D|\text{*},\text{*})q(N|\text{*},D)q(V|D,N)q(\text{STOP}|N,V)e(\text{the}|D)e(\text{dog}|N)e(\text{laughs}|V)<br>$$<br><strong>Why the name</strong><br>The product of the $q$ terms $\prod_{i=1}^{n+1}q(y_i|y_{i-2},y_{i-1})$ is essentially a prior probability over tag sequences $p(y_1\cdots y_{n+1})$, which is a second-order Markov chain. $\prod_{i=1}^ne(x_i|y_i)$ is essentially a condition probability $p(x_1\cdots x_n|y_1\cdots y_{n+1})$, where we assume that each word $x_i$ is chosen only depending on the value for $y_i$. The two assumptions are actually rather strong. We can think of a process that we first generate a tag sequence using the Markov chain, and for each tag we generate an associated word. The $x_i$’s are observed, and the generation of the tag sequence is unobserved (the Markov chain is hidden).</p>
<h3 id="Parameter-estimation"><a href="#Parameter-estimation" class="headerlink" title="Parameter estimation"></a>Parameter estimation</h3><p><strong>Smoothed Estimation</strong><br>Linear Interpolation<br>$$<br>q(\text{Vt|Dt,JJ})=\lambda_1\times\frac{\text{Count(Dt,JJ,Vt)}}{\text{Count(Dt,JJ)}}+\lambda_2\times\frac{\text{Count(JJ,Vt)}}{\text{Count(JJ)}}+\lambda_3\times\frac{\text{Count(Vt)}}{\text{Count()}}\\<br>\lambda_1+\lambda_2+\lambda_3=1, \text{and for all }i, \lambda_i\ge 0<br>$$<br><strong>Dealing with Low-Frequency Words</strong><br>$$<br>e(x|y)=\frac{\text{Count(x,y)}}{\text{Count(y)}}<br>$$<br>If $x$ is never seen in the training data, $e(x|y)=0$ for all $y$, which will break down the model. Consider a test sentence as follows: <i>Profits soared at Boeing Co. , easily topping forecasts on Wall Street , as their CEO Alan Mulally announced first quarter results.</i><br>It’s highly possible that the word <i>Mulally</i> is never seen in the training corpus. $e(\text{Mulally}|y)=0$ for all tags $y$, so $p(x_1\cdots x_n,y_1\cdots y_{n+1})=0$ for all tag sequences.</p>
<p>A common method is as follows:<br>Step 1: Split vocabulary into two sets<br><strong>Frequent words</strong> = words occurring 5 times in training<br><strong>Low frequency words</strong> = all other words<br>Step 2: Map low frequency words into a small, finite set, depending on prefixes, suffixes etc.</p>
<p>An example. These classes were chosen by hand using some intuition and insight of the problem. This partition was chosen to try to preserve some important information.<br><img src="..\img\nlp\low_frequency_class.png"><br><img src="..\img\nlp\low_frequency_mapping.png"></p>
<h3 id="The-Viterbi-Algorithm"><a href="#The-Viterbi-Algorithm" class="headerlink" title="The Viterbi Algorithm"></a>The Viterbi Algorithm</h3><p>How to use the trained model to predict test sentences?</p>
<p><strong>Problem:</strong> for a input $x_1\cdots x_n$, find $\arg\max_{y_1\cdots y_{n+1}}p(x_1\cdots x_n,y_1\cdots y_{n+1})$, where the $\arg \max$ is taken over all sequences $y_1\cdots y_{n+1}$ such that $y_i\in\mathcal S$ for $i=1,\cdots, n$ and $y_{n+1}=\text{STOP}$. We assume that $p$ takes the form below, where we have assumed in definition that $y_0=y_{-1}=*$, and $y_{n+1}=\text{STOP}$.</p>
<p>$$<br>p(x_1\cdots x_n,y_1\cdots y_{n+1})=\prod_{i=1}^{i+1}q(y_i|y_{i-2},y_{i-1})\prod_{i=1}^ne(x_i|y_i)<br>$$<br>Brute Force Search is hopelessly inefficient because the number of possible sequence is $|\mathcal S|^n$. As the length of sentence increases, it would cost enormous time to calculate the joint probability for every tag sequence and return the maximum.</p>
<p>Define $n$ to be the length of the sentence<br>Define $S_k$ for $k=-1,\cdots n$ to be the set of possible tags at position $k$: $S_{-1}=S_0=\{*\},S_k=S$ for $k\in\{1\cdots n\}$. <br>Define $r(y_{-1},y_0,y_1,\cdots,y_k)=\prod_{i=1}^kq(y_i|y_{i-2},y_{i-1})\prod_{i=1}^ke(x_i|y_i)$</p>
<p>Define a dynamic programming table $\pi(k,u,v)=\max_{\langle y_{-1},y_0,y_1,\cdots,y_k\rangle:y_{k-1}=u,y_k=v}r(y_{-1},y_0,y_1,\cdots y_k)$. It is the maximum probability of a tag sequence ending in tags $u,v$ at position $k$. Base case: $\pi(0,\text{*},\text{*})=1$. </p>
<p>Recursive definition:<br>$$<br>\pi(k,u,v)=\max_{w\in\mathcal S_{k-2}}\pi(k,w,u,v)=\max_{w\in\mathcal S_{k-2}} (\pi(k-1,w,u)\times q(v|w,u)\times e(x_k|v))<br>$$<br><strong>Input:</strong> a sentence $x_1\cdots x_n$, parameters $q(s|u,v)$ and $e(x|s)$.<br><strong>Initialization:</strong> set $\pi(0,\text{*},\text{*})=1$<br><strong>Definition:</strong> $S_{-1}=S_0=\{\text{*}\},S_k=S$ for $k\in\{1\cdots n\}$.<br><strong>Algorithm:</strong><br>    $\quad$For $k=1\cdots n$,<br>        $\quad$$\quad$For $u\in\mathcal S_{k-1},v\in\mathcal S_k,$<br>            $\quad$$\quad$$\quad$record $\pi(k,u,v)=\max_{w\in\mathcal S_{k-2}} (\pi(k-1,w,u)\times q(v|w,u)\times e(x_k|v))$<br>            $\quad$$\quad$$\quad$record $bp(k,u,v)=\arg\max_{w\in\mathcal S_{k-2}} (\pi(k-1,w,u)\times q(v|w,u)\times e(x_k|v))$<br>     $\quad$Set $(y_{n-1},y_n)=\arg\max_{(u,v)}(\pi(n,u,v)\times q(\text{STOP|u,v}))$<br>    $\quad$For $k=(n-2)\cdots1,y_k=bp(k+2,y_{k+1},y_{k+2})$<br>    $\quad$Return $y_1\cdots y_n$</p>
<p>Running time: $n|\mathcal S|^2​$ entries in $\pi​$ to be filled in. $O(|\mathcal S|)​$ time to fill in one entry. $O(n|\mathcal S|^3)​$ time in total.</p>
<h3 id="Pros-and-Cons"><a href="#Pros-and-Cons" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h3><p>Hidden Markov model taggers are very simple to train (just need to compile counts from the training corpus) and perform relatively well (over 90% performance on named entity recognition). The main difficulty is modeling $e(\text{word}|\text{tag})$, which can be very difficult if “words” are complex.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp2-1/" itemprop="url">
                  The Tagging Problem
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-23T05:27:39-05:00" content="2018-01-23">
              2018-01-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="The-tagging-problem"><a href="#The-tagging-problem" class="headerlink" title="The tagging problem"></a>The tagging problem</h3><p>Part-of-Speech tagging takes the sentence as input and to word by word assign a part of speech to each word in that input. Ambiguity is the main challenge because a word can serve as different parts of speech.</p>
<p>Named entity recognition takes sentences as input and marks up all the entities in that sentence in the output.<br><img src="..\img\nlp\named_entity.png"></p>
<p>Our goal is to induce from the training set a function/algorithm that maps new sentences to their tag sequences. There are two types of constraints. “Local”: e.g., <i>can</i> is more likely to be a modal verb MD<br>rather than a noun NN. “Contextual”: e.g., a noun is much more likely than a verb to follow a determiner<br>But sometimes these preferences are in conflict: <i>The trash can is in the garage</i>.</p>
<h3 id="Supervised-learning-problems"><a href="#Supervised-learning-problems" class="headerlink" title="Supervised learning problems"></a>Supervised learning problems</h3><p>Conditional models:<br>Learn a distribution $p(y|x)$ from training examples<br>For any test input $x$, define $f(x) = \operatorname{argmax}_y p(y|x)$</p>
<p>Generative models:<br>Learn a distribution $p(x,y)​$ from training examples<br>Often we have $p(x,y) = p(y)p(x|y)​$. $\qquad​$($p(y)​$:prior,$p(x|y)​$: conditional generative model)<br>Output $f(x)=\operatorname{argmax}_y p(y)p(x|y)​$<br>$$<br>\begin{align}<br>f(x) &amp;= \operatorname{argmax}_y p(y|x)\\<br>&amp;=\operatorname{argmax}_y \frac{p(y)p(x|y)}{p(x)}\\<br>&amp;=\operatorname{argmax}_y p(y)p(x|y)<br>\end{align}<br>$$<br>$p(y|x)$ directly: discriminate models</p>
<p>$p(x,y)$: generative models</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://zccz14.com/images/avatar.png"
               alt="dada" />
          <p class="site-author-name" itemprop="name">dada</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">21</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Captain-X/" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://zccz14.com/" title="zccz14" target="_blank">zccz14</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">dada</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
