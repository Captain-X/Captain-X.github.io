<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="Vinci">
<meta property="og:url" content="https://Captain-X.github.io/index.html">
<meta property="og:site_name" content="Vinci">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Vinci">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="https://Captain-X.github.io/"/>

  <title> Vinci </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Vinci</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp8-2/" itemprop="url">
                  Log-Linear Models for History-based Parsing
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-02-02T04:36:57-05:00" content="2018-02-02">
              2018-02-02
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp8-1/" itemprop="url">
                  Log-Linear Models for Tagging(MEMMs)
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-02-02T04:36:56-05:00" content="2018-02-02">
              2018-02-02
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp7-1/" itemprop="url">
                  Log-Linear Models
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-30T09:47:09-05:00" content="2018-01-30">
              2018-01-30
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp4-1/" itemprop="url">
                  Lexicalized PCFGs
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-29T23:19:45-05:00" content="2018-01-29">
              2018-01-29
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>###</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp3-2/" itemprop="url">
                  Probabilistic Context-Free Grammars
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-25T09:43:15-05:00" content="2018-01-25">
              2018-01-25
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>###Probabilistic Context-Free Grammars (PCFGs)</p>
<p>Under PCFG, each rule is assigned with a probability.<br><img src="../img/nlp/pcfg.png"></p>
<p>Probability of a tree $t$ with rules $\alpha_1\rightarrow\beta_1,\alpha_2\rightarrow\beta_2,\cdots, \alpha_n\rightarrow\beta_n$ is $p(t)=\prod_{i=1}^n q(\alpha_1\rightarrow\beta_1)$. A PCFG assigns a probability to each left-most derivation, or parse-tree, allowed by the underlying CFG. Say we have a sentence $s$, set of derivations for that sentence is $\mathcal T(s)$. Then a PCFG assigns a probability $p(t)$ to each member of $\mathcal T(s)$. i.e., we now have a ranking in order of probability. The most likely parse tree for a sentence $s$ is $\arg \max_{t\in \mathcal T(s)} p(t)$.</p>
<p><strong>Deriving a PCFG from a Treebank</strong><br>Given a set of example trees (a treebank), the underlying CFG can simply be all rules seen in the corpus<br>Maximum Likelihood estimates for $q_(\alpha\rightarrow\beta)$:<br>$$q_{ML}(\alpha\rightarrow\beta) = \frac{\text{Count}(\alpha\rightarrow\beta)}{\text{Count}(\alpha)}$$<br>where the counts are taken from a training set of example trees.<br>If the training data is generated by a PCFG, then as the training data size goes to infinity, the maximum-likelihood PCFG will converge to the same distribution as the”true” PCFG.</p>
<h3 id="The-CKY-Algorithm"><a href="#The-CKY-Algorithm" class="headerlink" title="The CKY Algorithm"></a>The CKY Algorithm</h3><h4 id="Chomsky-Normal-Form"><a href="#Chomsky-Normal-Form" class="headerlink" title="Chomsky Normal Form"></a>Chomsky Normal Form</h4><p>A context free grammar $G = (N,\Sigma,R,S)$ in Chomsky Normal Form is as follows:<br>$\qquad N$ is a set of non-terminal symbols<br>$\qquad\Sigma$ is a set of terminal symbols<br>$\qquad R$ is a set of rules which take one of two forms:<br>$\qquad\qquad X \rightarrow Y_1Y_2$ for $X\in N$, and $Y_1,Y_2\in N$<br>$\qquad\qquad X \rightarrow Y $ for $X \in N$, and $Y\in\Sigma$ <br>$\qquad S\in N$ is a distinguished start symbol<br></p>
<p>It seems that the form has put many restrictions to the grammar, but in fact any PCFG can be converted into a CFG in Chomsky Normal Form by some tricks. e.g., $\{\text{VP}\rightarrow \text{Vt}\quad \text{NP}\quad \text{PP}\} (0.2)$ can be converted to $\{\text{VP}\rightarrow \text{Vt-NP}\quad \text{PP}\} (0.2)$ and $\{\text{Vt-NP}\rightarrow \text{Vt}\quad \text{NP}\} (1.0)$</p>
<p>A Dynamic Programming Algorithm<br>Given a PCFG in Chomsky Normal Form and a sentence $s$, how do we find $\arg\max_{t\in\mathcal T(s) }p(t)$<br>Notation:<br>$n$ = number of words in the sentence<br>$w_i$= i’th word in the sentence<br>$N$ = the set of non-terminals in the grammar<br>$S$ = the start symbol in the grammar<br>Define a dynamic programming table<br>$\pi[i,j,X]$ = maximum probability of a constituent with non-terminal X spanning words $i,\cdots,j$ inclusive.<br>Our goal is to calculate $\max_{t\in\mathcal T(s) }p(t)=\pi[1,n,S]$.</p>
<p>We are searching through all different decompositions, that is, search for all different choices of rules and all different choices of splitting point, and will thereby find the single best way of reaching $X$ spanning $w_i$ to $w_j$</p>
<p><strong>Input:</strong> a sentence $s$ = $x_1\cdots x_n$, a PCFG $G = (N,\Sigma,S,R,q)$.<br><strong>Initialization:</strong> For all $i\in\{1,\cdots,n\}$, for all $X\in N$, $\pi(i,i,X)=q(X\rightarrow x_i)$<br><strong>Algorithm:</strong><br>For $l =1,\cdots, (n-1)$<br>For $i = 1,\cdots,(n-l)$<br>Set $j = i + l$<br>For all $X \in N$, calculate<br>$$<br>\pi(i,j,X)=\underset{s\in\{i,\cdots,(j-1)\}}{\underset{(X\rightarrow YZ)\in R}{\max}} (q(X\rightarrow YZ)\times\pi(i,s,Y)\times\pi(s+1,j,Z))\\<br>bp(i,j,X)=\underset{s\in\{i,\cdots,(j-1)\}}{\underset{(X\rightarrow YZ)\in R}{\arg\max}} (q(X\rightarrow YZ)\times\pi(i,s,Y)\times\pi(s+1,j,Z))\\<br>$$</p>
<h3 id="Weaknesses-of-PCFGs"><a href="#Weaknesses-of-PCFGs" class="headerlink" title="Weaknesses of PCFGs"></a>Weaknesses of PCFGs</h3><h4 id="Lack-of-sensitivity-to-lexical-information"><a href="#Lack-of-sensitivity-to-lexical-information" class="headerlink" title="Lack of sensitivity to lexical information"></a>Lack of sensitivity to lexical information</h4><p>Example 1</p>
<p><img src="../img/nlp/weakness_eg1.png"><br>Consider a word in the tree, once we condition on the non-terminal above that word, i.e., part of speech, the assumption in PCFG is that the choice of this word is conditionally independent of everything else in the tree, which means we are making an assumption that the part of speech carries all the information we could possibly need about the identity of this word. It is a very strong and bad assumption</p>
<p>Example 2 (PP Attachment Ambiguity)</p>
<p><img src="../img/nlp/weakness_eg2.png"><img src="../img/nlp/weakness_eg2a.png"><br>The parsing of every sentence in this syntactic structure (specifically the attachment of PP), depends only on between $q(VP\rightarrow VP\quad PP)$ and $q(NP\rightarrow NP\quad PP)$ which is greater. This decision is very sub-optimal because it is completely based on the prior of the parse trees and ignores the lexical information lied in these phrases. It is unable to distinguish the two different attachments, classifying them into the same parse tree if two probabilities are already obtained. <strong>Attachment decision is completely independent of the words.</strong></p>
<p>Example 3 (Coordination Ambiguity)</p>
<p><img src="../img/nlp/weakness_eg3.png"><img src="../img/nlp/weakness_eg3a.png"><br>Here the two parses have identical rules, and therefore have identical probability under any assignment of PCFG rule probabilities</p>
<h4 id="Lack-of-sensitivity-to-structural-frequencies"><a href="#Lack-of-sensitivity-to-structural-frequencies" class="headerlink" title="Lack of sensitivity to structural frequencies"></a>Lack of sensitivity to structural frequencies</h4><p>Example: president of a company in Africa (PP attachment for noun)<br><img src="../img/nlp/weakness_eg4.png"><br>Both parses have the same rules, therefore receive same probability under a PCFG. “Close attachment” (structure (a)) is twice as likely in Wall Street Journal text. An even more pronounced of close attachment preference is attachment for verbs.e.g, <i>John was believed to have been shot by Bill</i>. Close attachment preference in case of verbs are approximately 20 times more likely.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp3-1/" itemprop="url">
                  Parsing and Context-Free Grammars
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-24T04:18:14-05:00" content="2018-01-24">
              2018-01-24
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>###The parsing problem</p>
<p>A parse tree is a hierarchical decomposition of a sentence, encoding the grammatical structure within the sentence.</p>
<p><strong>The information conveyed by parse trees</strong></p>
<p>(1) Part of speech for each word (N = noun, V = verb, DT = determiner)<br><img src="../img/nlp/parse_info.png"></p>
<p>(2) Phrases<br>Noun Phrases (NP): “the burglar”, “the apartment”<br>Verb Phrases (VP): “robbed the apartment”<br>Sentences (S): “the burglar robbed the apartment”</p>
<p>(3) Useful Relationships<br><img src="../img/nlp/parse_relation.png"><br>This template tells us that “the burglar” is the subject of “robbed”</p>
<p>###Context-free grammars</p>
<p>A context free grammar $G = (N,\Sigma,R,S)$ where:<br>$\qquad N$ is a set of non-terminal symbols<br>$\qquad\Sigma$ is a set of terminal symbols<br>$\qquad R$ is a set of rules of the form $X\rightarrow Y_1Y_2\cdots Y_n$ for $n\ge0,X\in N,Y_i\in(N\cup \Sigma)$<br>$\qquad S\in N$ is a distinguished start symbol</p>
<p><strong>An example</strong><br><img src="../img/nlp/cfg.png"></p>
<p>####Left-most deriviation</p>
<p>A left-most derivation is a sequence of strings $s_1\cdots s_n$, where $s_1=S$, the start symbol, $s_n\in\Sigma\text{*}$, i.e., $s_n$ is made up of terminal symbols only. Each $s_i$ for $i=2\cdots n $ is derived from $s_{i-1}$ by picking the left-most non-terminal $X$ in $s_{i-1}$ and replacing it by some $\beta$ where $X\rightarrow\beta$ is a rule in $R$. For example: [S], [NP VP], [D N VP], [the N VP], [the man VP], [the man Vi], [the man sleeps].</p>
<p>A CFG defines a set of possible derivations. (In most cases the set is infinite) A string $s\in\Sigma\text{*}$ is in the language defined by the CFG if there is at least one derivation that yields $s$. Each string in the language generated by the CFG may have more than one derivation (“ambiguity”). For example, the sentence “he drove down the street in the car” can be derived from:<br>[S], [NP VP], [he VP], <strong>[he VP PP], [he VB PP PP], [he drove PP PP], [he drove IN NP PP]</strong>, [he drove down NP PP], [he drove down DT NN PP], [he drove down the NN PP], [he drove down the street PP], [he drove down the street IN NP], [he drove down the street in NP], [he drove down the street in DT NN], [he drove down the street in the NN], [he drove down the street in the car]<br>[S], [NP VP], [he VP], <strong>[he VB PP], [he drove PP], [he drove IN NP], [he drove down NP]</strong>, [he drove down NP PP], [he drove down DT NN PP], [he drove down the NN PP], [he drove down the street PP], [he drove down the street IN NP], [he drove down the street in NP], [he drove down the street in DT NN], [he drove down the street in the NN], [he drove down the street in the car]</p>
<p>We use parse tree to represent these deriviations. Different parse trees correspond to difference interpretations of the underlying sentence. In the first parse tree, the PP “in the car” is modifying the entire verb phrase “drove down the street” (by far the most likely interpretation), while in the second tree, it is modifying the noun phrase “the street” (which means the street is in the car).<br><img src="../img/nlp/deriviation2.png"><img src="../img/nlp/deriviation1.png"></p>
<p>The problem with parsing—ambiguity: there are several possible syntactic structures for the sentence corresponding to various different interpretations, most of which are quite unlikely but all of which are nevertheless sytactically well formed.</p>
<p>###A brief sketch of the syntax of English</p>
<p>####Noun phrases</p>
<p>(NN = singular noun, NNS = plural noun, NNP = proper noun, DT = determiner, JJ = adjective)<br>A fragment of noun phrase grammar<br><img src="../img/nlp/noun_phrase1.png"><br><img src="../img/nlp/noun_phrase2.png"></p>
<p>####Prepositional phrases</p>
<p>An extended grammar<br>(IN = preposition)<br><img src="../img/nlp/pp1.png"><br><img src="../img/nlp/pp2.png"></p>
<h4 id="Verbs-verb-phrases-and-sentences"><a href="#Verbs-verb-phrases-and-sentences" class="headerlink" title="Verbs, verb phrases, and sentences"></a>Verbs, verb phrases, and sentences</h4><p>Basic Verb Types<br>$\qquad$Vi = Intransitive verb<br>$\qquad$Vt = Transitive verb<br>$\qquad$Vd = Ditransitive verb (give)<br>Basic VP Rules<br>$\qquad$VP $\rightarrow$ Vi<br>$\qquad$VP $\rightarrow$ Vt NP<br>$\qquad$VP $\rightarrow$ Vd NP NP (give the dog a ball)<br>Basic S Rule<br>$\qquad$S  $\rightarrow$ NP VP</p>
<p>PPs Modifying Verb Phrases<br>$\qquad$VP $\rightarrow$ VP PP (gave the dog a ball on Wednesday)</p>
<p>####Complementizers, and SBARs</p>
<p>COMP = complementizer (e.g., that)<br>SBAR $\rightarrow$ COMP S</p>
<p>New Verb Types<br>$\qquad$V[5] (e.g., said, reported)<br>$\qquad$V[6] (e.g., told, informed)<br>$\qquad$V[7] (e.g., bet)<br>New VP Rules<br>$\qquad$VP  $\rightarrow$  V[5] SBAR  (said that the man sleeps)<br>$\qquad$VP  $\rightarrow$  V[6] NP SBAR  (told the dog that the mechanic likes the pigeon)<br>$\qquad$VP  $\rightarrow$  V[7] NP NP SBAR  (bet the pigeon $50 that the mechanic owns a fast car)</p>
<p>####Coordination</p>
<p>A New Part-of-Speech:<br>$\qquad$CC = Coordinator (e.g., and, or, but)<br>New Rules<br>$\qquad$NP $\rightarrow$ NP CC NP<br>$\qquad$$\bar N$ $\rightarrow$ $\bar N$ CC $\bar N$<br>$\qquad$VP $\rightarrow$ VP CC VP<br>$\qquad$S $\rightarrow$ S CC S<br>$\qquad$SBAR $\rightarrow$ SBAR CC SBAR</p>
<h4 id="Only-a-scratch-of-the-surface"><a href="#Only-a-scratch-of-the-surface" class="headerlink" title="Only a scratch of the surface!"></a>Only a scratch of the surface!</h4><p><strong>Agreement</strong>: The dogs laugh v.s. The dog laughs<br><strong>Wh-movement</strong>: The dog that the cat liked … (The direct object of like is the dog instead of …)<br><strong>Active v.s. passive</strong>: The dog saw the cat v.s. The cat was seen by the dog</p>
<p>###Examples of ambiguous structures</p>
<p>####Part-of-Speech ambiguity</p>
<p>The word “duck” can serve as both NN and Vi. This kind of ambiguity frequently leads to multiple possible parse structures for a particular sentence.<br><img src="../img/nlp/part_ambiguity.png"></p>
<p>####Prepositional phrase attachment</p>
<p><img src="../img/nlp/deriviation2.png"><img src="../img/nlp/deriviation1.png"></p>
<p>Another example: John was believed to have been shot by Bill, which can be interpreted as Bill shot John or Bill believe John has been shot. But the former is more plausible due to a strong preference for PP to modify the most recent verb.</p>
<p>####Noun Premodifier</p>
<p><img src="../img/nlp/noun_premodifier.png"><br>The JJ “fast” can modify “car” or “mechanic”. </p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp2-2/" itemprop="url">
                  Hidden Markov Model Taggers
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-23T09:34:04-05:00" content="2018-01-23">
              2018-01-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Basic-definitions"><a href="#Basic-definitions" class="headerlink" title="Basic definitions"></a>Basic definitions</h3><p>We have an input sentence $x=x_1,x_2,\cdots,x_n$, and a tag sequence $y=y_1,y_2,\cdots,y_n$. We use an Hidden Markov Model to define a joint probability $p(x_1,x_2,\cdots,x_n,y_1,y_2,\cdots,y_n)$ for any sentence $x_1,x_2,\cdots,x_n$ and tag sequence $y_1,y_2,\cdots,y_n$ of the same length. Then the most likely tag sequence for $x$ is $\operatorname{argmax}_{y_1\cdots y_n}p(x_1\cdots x_n,y_1\cdots y_n)$</p>
<h4 id="Trigram-Hidden-Markov-Models"><a href="#Trigram-Hidden-Markov-Models" class="headerlink" title="Trigram Hidden Markov Models"></a>Trigram Hidden Markov Models</h4><p>We define $\mathcal V$ is the set of all possible vocabulary, and $\mathcal S$ is the set of all possible tags ($|\mathcal S|\approx50$). For any sentence $x_1\cdots x_n$ where $x_i\in\mathcal V$ , and any tag sequence $y_1\cdots y_{n+1}$ where $y_i\in \mathcal S$ and $y_{n+1}=\text{STOP}$, the joint probability of the sentence and tag sequence is<br>$$<br>p(x_1\cdots x_n,y_1\cdots y_{n+1})=\prod_{i=1}^{n+1}q(y_i|y_{i-2},y_{i-1})\prod_{i=1}^ne(x_i|y_i)\\<br>$$<br>Parameters of the model:<br>$q(s|u,v)$ for any $s\in\mathcal S\cup\{\text{STOP}\},u,v\in\mathcal S\cup\{*\}$ (trigram parameter)<br>$e(x|s) $ for any $s\in\mathcal S,x\in\mathcal V$ (emission parameter)</p>
<p><strong>An example</strong><br>If we have $n=3$, $x_1\cdots x_3$ equal to the sentence <i>the dog laughs</i>, and $y_1\cdots y_4$ equal to the tag sequence $\text{D N V STOP}$, then<br>$$<br>p(x_1\cdots x_n,y_1\cdots y_{n+1})=q(D|\text{*},\text{*})q(N|\text{*},D)q(V|D,N)q(\text{STOP}|N,V)e(\text{the}|D)e(\text{dog}|N)e(\text{laughs}|V)<br>$$<br><strong>Why the name</strong><br>The product of the $q$ terms $\prod_{i=1}^{n+1}q(y_i|y_{i-2},y_{i-1})$ is essentially a prior probability over tag sequences $p(y_1\cdots y_{n+1})$, which is a second-order Markov chain. $\prod_{i=1}^ne(x_i|y_i)$ is essentially a condition probability $p(x_1\cdots x_n|y_1\cdots y_{n+1})$, where we assume that each word $x_i$ is chosen only depending on the value for $y_i$. The two assumptions are actually rather strong. We can think of a process that we first generate a tag sequence using the Markov chain, and for each tag we generate an associated word. The $x_i$’s are observed, and the generation of the tag sequence is unobserved (the Markov chain is hidden).</p>
<h3 id="Parameter-estimation"><a href="#Parameter-estimation" class="headerlink" title="Parameter estimation"></a>Parameter estimation</h3><p><strong>Smoothed Estimation</strong><br>Linear Interpolation<br>$$<br>q(\text{Vt|Dt,JJ})=\lambda_1\times\frac{\text{Count(Dt,JJ,Vt)}}{\text{Count(Dt,JJ)}}+\lambda_2\times\frac{\text{Count(JJ,Vt)}}{\text{Count(JJ)}}+\lambda_3\times\frac{\text{Count(Vt)}}{\text{Count()}}\\<br>\lambda_1+\lambda_2+\lambda_3=1, \text{and for all }i, \lambda_i\ge 0<br>$$<br><strong>Dealing with Low-Frequency Words</strong><br>$$<br>e(x|y)=\frac{\text{Count(x,y)}}{\text{Count(y)}}<br>$$<br>If $x$ is never seen in the training data, $e(x|y)=0$ for all $y$, which will break down the model. Consider a test sentence as follows: <i>Profits soared at Boeing Co. , easily topping forecasts on Wall Street , as their CEO Alan Mulally announced first quarter results.</i><br>It’s highly possible that the word <i>Mulally</i> is never seen in the training corpus. $e(\text{Mulally}|y)=0$ for all tags $y$, so $p(x_1\cdots x_n,y_1\cdots y_{n+1})=0$ for all tag sequences.</p>
<p>A common method is as follows:<br>Step 1: Split vocabulary into two sets<br><strong>Frequent words</strong> = words occurring 5 times in training<br><strong>Low frequency words</strong> = all other words<br>Step 2: Map low frequency words into a small, finite set, depending on prefixes, suffixes etc.</p>
<p>An example. These classes were chosen by hand using some intuition and insight of the problem. This partition was chosen to try to preserve some important information.<br><img src="..\img\nlp\low_frequency_class.png"><br><img src="..\img\nlp\low_frequency_mapping.png"></p>
<h3 id="The-Viterbi-Algorithm"><a href="#The-Viterbi-Algorithm" class="headerlink" title="The Viterbi Algorithm"></a>The Viterbi Algorithm</h3><p>How to use the trained model to predict test sentences?</p>
<p><strong>Problem:</strong> for a input $x_1\cdots x_n$, find $\arg\max_{y_1\cdots y_{n+1}}p(x_1\cdots x_n,y_1\cdots y_{n+1})$, where the $\arg \max$ is taken over all sequences $y_1\cdots y_{n+1}$ such that $y_i\in\mathcal S$ for $i=1,\cdots, n$ and $y_{n+1}=\text{STOP}$. We assume that $p$ takes the form below, where we have assumed in definition that $y_0=y_{-1}=*$, and $y_{n+1}=\text{STOP}$.</p>
<p>$$<br>p(x_1\cdots x_n,y_1\cdots y_{n+1})=\prod_{i=1}^{i+1}q(y_i|y_{i-2},y_{i-1})\prod_{i=1}^ne(x_i|y_i)<br>$$<br>Brute Force Search is hopelessly inefficient because the number of possible sequence is $|\mathcal S|^n$. As the length of sentence increases, it would cost enormous time to calculate the joint probability for every tag sequence and return the maximum.</p>
<p>Define $n$ to be the length of the sentence<br>Define $S_k$ for $k=-1,\cdots n$ to be the set of possible tags at position $k$: $S_{-1}=S_0=\{*\},S_k=S$ for $k\in\{1\cdots n\}$. <br>Define $r(y_{-1},y_0,y_1,\cdots,y_k)=\prod_{i=1}^kq(y_i|y_{i-2},y_{i-1})\prod_{i=1}^ke(x_i|y_i)$</p>
<p>Define a dynamic programming table $\pi(k,u,v)=\max_{\langle y_{-1},y_0,y_1,\cdots,y_k\rangle:y_{k-1}=u,y_k=v}r(y_{-1},y_0,y_1,\cdots y_k)$. It is the maximum probability of a tag sequence ending in tags $u,v$ at position $k$. Base case: $\pi(0,\text{*},\text{*})=1$. </p>
<p>Recursive definition:<br>$$<br>\pi(k,u,v)=\max_{w\in\mathcal S_{k-2}}\pi(k,w,u,v)=\max_{w\in\mathcal S_{k-2}} (\pi(k-1,w,u)\times q(v|w,u)\times e(x_k|v))<br>$$<br><strong>Input:</strong> a sentence $x_1\cdots x_n$, parameters $q(s|u,v)$ and $e(x|s)$.<br><strong>Initialization:</strong> set $\pi(0,\text{*},\text{*})=1$<br><strong>Definition:</strong> $S_{-1}=S_0=\{\text{*}\},S_k=S$ for $k\in\{1\cdots n\}$.<br><strong>Algorithm:</strong><br>    $\quad$For $k=1\cdots n$,<br>        $\quad$$\quad$For $u\in\mathcal S_{k-1},v\in\mathcal S_k,$<br>            $\quad$$\quad$$\quad$record $\pi(k,u,v)=\max_{w\in\mathcal S_{k-2}} (\pi(k-1,w,u)\times q(v|w,u)\times e(x_k|v))$<br>            $\quad$$\quad$$\quad$record $bp(k,u,v)=\arg\max_{w\in\mathcal S_{k-2}} (\pi(k-1,w,u)\times q(v|w,u)\times e(x_k|v))$<br>     $\quad$Set $(y_{n-1},y_n)=\arg\max_{(u,v)}(\pi(n,u,v)\times q(\text{STOP|u,v}))$<br>    $\quad$For $k=(n-2)\cdots1,y_k=bp(k+2,y_{k+1},y_{k+2})$<br>    $\quad$Return $y_1\cdots y_n$</p>
<p>Running time: $n|\mathcal S|^2​$ entries in $\pi​$ to be filled in. $O(|\mathcal S|)​$ time to fill in one entry. $O(n|\mathcal S|^3)​$ time in total.</p>
<h3 id="Pros-and-Cons"><a href="#Pros-and-Cons" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h3><p>Hidden Markov model taggers are very simple to train (just need to compile counts from the training corpus) and perform relatively well (over 90% performance on named entity recognition). The main difficulty is modeling $e(\text{word}|\text{tag})$, which can be very difficult if “words” are complex.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp2-1/" itemprop="url">
                  The Tagging Problem
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-23T05:27:39-05:00" content="2018-01-23">
              2018-01-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="The-tagging-problem"><a href="#The-tagging-problem" class="headerlink" title="The tagging problem"></a>The tagging problem</h3><p>Part-of-Speech tagging takes the sentence as input and to word by word assign a part of speech to each word in that input. Ambiguity is the main challenge because a word can serve as different parts of speech.</p>
<p>Named entity recognition takes sentences as input and marks up all the entities in that sentence in the output.<br><img src="..\img\nlp\named_entity.png"></p>
<p>Our goal is to induce from the training set a function/algorithm that maps new sentences to their tag sequences. There are two types of constraints. “Local”: e.g., <i>can</i> is more likely to be a modal verb MD<br>rather than a noun NN. “Contextual”: e.g., a noun is much more likely than a verb to follow a determiner<br>But sometimes these preferences are in conflict: <i>The trash can is in the garage</i>.</p>
<h3 id="Supervised-learning-problems"><a href="#Supervised-learning-problems" class="headerlink" title="Supervised learning problems"></a>Supervised learning problems</h3><p>Conditional models:<br>Learn a distribution $p(y|x)$ from training examples<br>For any test input $x$, define $f(x) = \operatorname{argmax}_y p(y|x)$</p>
<p>Generative models:<br>Learn a distribution $p(x,y)​$ from training examples<br>Often we have $p(x,y) = p(y)p(x|y)​$. $\qquad​$($p(y)​$:prior,$p(x|y)​$: conditional generative model)<br>Output $f(x)=\operatorname{argmax}_y p(y)p(x|y)​$<br>$$<br>\begin{align}<br>f(x) &amp;= \operatorname{argmax}_y p(y|x)\\<br>&amp;=\operatorname{argmax}_y \frac{p(y)p(x|y)}{p(x)}\\<br>&amp;=\operatorname{argmax}_y p(y)p(x|y)<br>\end{align}<br>$$<br>$p(y|x)$ directly: discriminate models</p>
<p>$p(x,y)$: generative models</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp1-3/" itemprop="url">
                  Parameter Estimation in Language Models
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-23T05:27:28-05:00" content="2018-01-23">
              2018-01-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Linear-Interpolation"><a href="#Linear-Interpolation" class="headerlink" title="Linear Interpolation"></a>Linear Interpolation</h3><p>Trigram maximum likehood estimate has relatively low bias but needs very large dataset to accurately estimate.<br>$$<br>q_{ML}(w_i|w_{i-2},w_{i-1})=\frac{\text{Count}(w_{i-2},w_{i-1},w_i)}{\text{Count}(w_{i-2},w_{i-1})}<br>$$<br>Linear interpolation comes up with an estimate that takes into account the maximum likelihood estimates at the trigram, bigram and unigram levels, and brings 3 additional parameters $\lambda_1, \lambda_2, \lambda_3$.<br>$$<br>q(w_i|w_{i-2},w_{i-1})=\lambda_1\times q_{ML}(w_i|w_{i-2},w_{i-1})+\lambda_2\times q_{ML}(w_i|w_{i-1})+\lambda_3\times q_{ML}(w_i)\\<br>\text{where }\lambda_1+\lambda_2+ \lambda_3=1, \text{and }\lambda_i\ge0 \text{ so that}\sum_{w\in\mathcal V’}q(w|u,v)=1\text{ and }q(w|u,v)\ge0<br>$$<br>How to estimate $\lambda$?</p>
<p>We first hold out part of training set as validation data, then define $c’(w_1,w_2,w_3)$ to be the number of times the trigram $(w_1,w_2,w_3)$ is seen in validation set, choose $\lambda_1,\lambda_2,\lambda_3$ to maximize:<br>$$<br>L(\lambda_1,\lambda_2,\lambda_3)=\sum_{(w_1,w_2,w_3)}c’(w_1,w_2,w_3)\log q(w_3|w_1,w_2)<br>$$<br>Allowing $\lambda$ to vary</p>
<p>We take a function $\Pi$ that partitions histories. This means for different ranges of $\text{Count}(w_{i-2},w_{i-1})$, we should find different sets of $\lambda$’s. For example, when $\text{Count}(w_{i-2},w_{i-1})=0$, $\lambda_1^1$should be equal to 0 because the trigram estimate is undefined.<br>$$<br>\Pi(w_{i-2},w_{i-1})=<br>\begin{cases}<br>1, \text{Count}(w_{i-2},w_{i-1})=0\\<br>2,1\le\text{Count}(w_{i-2},w_{i-1})\le2\\<br>3,3\le\text{Count}(w_{i-2},w_{i-1})\le5\\<br>4,\text{otherwise}\\<br>\end{cases}<br>\\<br>q(w_i|w_{i-2},w_{i-1})=\lambda_1^{\displaystyle\Pi(w_{i-2},w_{i-1})} q_{ML}(w_i|w_{i-2},w_{i-1})+\lambda_2^{\displaystyle\Pi(w_{i-2},w_{i-1})} q_{ML}(w_i|w_{i-1})+\lambda_3^{\displaystyle\Pi(w_{i-2},w_{i-1})} q_{ML}(w_i)\\<br>\text{where }\lambda_1^{\displaystyle\Pi(w_{i-2},w_{i-1})}+\lambda_2^{\displaystyle\Pi(w_{i-2},w_{i-1})}+ \lambda_3^{\displaystyle\Pi(w_{i-2},w_{i-1})}=1, \text{and }\lambda_i^{\displaystyle\Pi(w_{i-2},w_{i-1})}\ge0<br>$$</p>
<h3 id="Discounting-methods"><a href="#Discounting-methods" class="headerlink" title="Discounting methods"></a>Discounting methods</h3><h4 id="Discounted-counts"><a href="#Discounted-counts" class="headerlink" title="Discounted counts"></a>Discounted counts</h4><p>Say we’ve seen the following counts:<br><img src="..\img\nlp\counts.png"></p>
<p>The maximum-likelihood estimates are high, particularly for low count items. (They get lucky because the denominator is small and only one appearance will lead to a relatively great value of frequency). Now define “discounted” counts: $\text{Count*}(x) = \text{Count}(x)-0.5$<br><img src="..\img\nlp\discounted.png"></p>
<p>We now have some “missing probability mass”:<br>$$<br>\alpha(w_{i-1})=1-\sum_w\frac{\text{Count*}(w_{i-1},w)}{\text{Count}(w_{i-1})}<br>$$<br>In our example, $\alpha(\text{the})=10\times0.5/48=5/48$</p>
<h4 id="Katz-Back-Off-Models-Bigrams"><a href="#Katz-Back-Off-Models-Bigrams" class="headerlink" title="Katz Back-Off Models (Bigrams)"></a>Katz Back-Off Models (Bigrams)</h4><p>For a bigram model, define two sets<br>$$<br>\mathcal A(w_{i-1})=\{w:\text{Count}(w_{i-1},w)&gt;0\}\\<br>\mathcal B(w_{i-1})=\{w:\text{Count}(w_{i-1},w)=0\}\\<br>$$<br>The bigram estimate is<br>$$<br>q_{BO}(w_i|w_{i-1})=<br>\begin{cases}<br>\displaystyle\frac{\text{Count*}(w_{i-1},w_i)}{\text{Count}(w_{i-1})}, &amp;\text{if }w_i\in\mathcal A(w_{i-1})\\<br>\alpha(w_{i-1})\displaystyle\frac{q_{ML}(w_i)}{\sum_{\displaystyle w\in\mathcal B(w_{i-1})}q_{ML}(w)},&amp; \text{if }w_i\in\mathcal B(w_{i-1})\\<br>\end{cases}<br>$$</p>
<h4 id="Katz-Back-Off-Models-Trigrams"><a href="#Katz-Back-Off-Models-Trigrams" class="headerlink" title="Katz Back-Off Models (Trigrams)"></a>Katz Back-Off Models (Trigrams)</h4><p>For a trigram model, define two sets<br>$$<br>\mathcal A(w_{i-2},w_{i-1})=\{w:\text{Count}(w_{i-2},w_{i-1},w)&gt;0\}\\<br>\mathcal B(w_{i-2},w_{i-1})=\{w:\text{Count}(w_{i-2},w_{i-1},w)=0\}\\<br>$$<br>The trigram estimate is<br>$$<br>q_{BO}(w_i|w_{i-2},w_{i-1})=<br>\begin{cases}<br>\displaystyle\frac{\text{Count*}(w_{i-2},w_{i-1},w_i)}{\text{Count}(w_{i-2},w_{i-1})}, &amp;\text{if }w_i\in\mathcal A(w_{i-2},w_{i-1})\\<br>\alpha(w_{i-2},w_{i-1})\displaystyle\frac{q_{BO}(w_i|w_{i-1})}{\sum_{\displaystyle w\in\mathcal B(w_{i-2},w_{i-1})}q_{BO}(w|w_{i-1})},&amp; \text{if }w_i\in\mathcal B(w_{i-2},w_{i-1})\\<br>\end{cases}<br>$$<br>When assigning the missing probability mass to unseen trigrams, the proportion of each trigram <strong>depends on the bigram BO estimate</strong>. (Iteration)</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>Three steps in deriving the language model probabilities:</p>
<ol>
<li>Expand $p(w_1,w_2,\cdots,w_n)$ using Chain rule.</li>
<li>Make Markov Independence Assumptions $p(w_ i| w_1,\cdots,w_{i-2},w_{i-1}) = p(w_ i|w_{i-2},w_{i-1}) $</li>
<li><strong>Smooth</strong> the estimates using low order counts.</li>
</ol>
<p>Other methods used to improve language models:<br>Conditioning on other more important context information such as “Topic” or “long-range” features<br>Basing on Syntactic models, explicitly trying to incorporate grammatical information. These models can often capture the long range features, which fall outside just a two-way window.</p>
<p>It’s generally hard to improve on trigram models though because they’re simple and very efficient.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp1-2/" itemprop="url">
                  Language Modeling
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-23T03:37:23-05:00" content="2018-01-23">
              2018-01-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Language-modeling-problem"><a href="#Language-modeling-problem" class="headerlink" title="Language modeling problem"></a>Language modeling problem</h3><p>We have some finite vocabulary, which includes all of the words in our language of interest, say $\mathcal V=$ {the, a, man, telescope, Beckham, two, …}</p>
<p>We have an (infinite) set of strings, $\mathcal V^\dagger$. A sentence could have any sequence of words, such as<br>the STOP<br>a STOP<br>the fan STOP<br>the fan saw Beckham STOP<br>the fan saw saw STOP<br>the fan saw Beckham play for Real Madrid STOP<br>STOP</p>
<p><strong>Given a training sample of example sentences, we need to learn a probability distribution.</strong></p>
<p>$p(\text{the STOP})=10^{-12}$<br>$p(\text{the fan STOP})=10^{-8}$<br>$p(\text{the fan saw Beckham STOP})=2\times10^{-8}$<br>$p(\text{the fan saw saw STOP})=10^{-15}$<br>$p(\text{the fan saw Beckham play for Real Madrid STOP})=2\times10^{-9}$</p>
<p>Speech recognition was the original motivation. Related problems are optical character recognition, handwriting recognition. The estimation techniques developed for this problem will be VERY useful for other problems in NLP.</p>
<p>From acoustic point of view, “recognize speech” and “wreck a nice beach” are quite similar. If we simply look at a measure of how compatible this sentence is with the acoustics sentence, it’s quite possible we might confuse these two sentences. In practice, there are many other possibilities which might have a reasonable degree of fit with the acoustic input and might be quite confusable with the true sentence. If we have a language model, we can actually evaluate a probability of each of these sentences. And a language model adds some very useful information to this whole process, which is the fact that the sentence, recognize speech, is probably more probable than the sentence wreck a nice beach. In practice, modern speech recognizers use two sources of information. Firstly, they have some way of evaluating how well each of these sentences match the input from an acoustic point of view. But secondly, they also have a language model which gives a essentially prior probability over the different sentences in the language. It can be very useful in getting rid of this kind of confusions.</p>
<h4 id="A-naive-method"><a href="#A-naive-method" class="headerlink" title="A naive method"></a>A naive method</h4><p>We have $N$ training sentences. For any sentence $x_1\cdots x_n$, $c(x_1\cdots x_n)$ is the number of times the sentence is seen in our training data. A naive estimate is<br>$$<br>p(x1\cdots x_n)=\frac{c(x1\cdots x_n)}{N}<br>$$<br>But it has some very clear deficiencies. Most importantly, it will assign probability 0 to any sentence not seen in our training sample. And we know that we’re continuously seeing new sentences in a language. So, this model has no ability to generalize to new sentences. The most important question is essentially, how can we build models to improve upon this naive estimate and in particular, models which generalize well to new test sentences.</p>
<h3 id="Markov-Processes"><a href="#Markov-Processes" class="headerlink" title="Markov Processes"></a>Markov Processes</h3><p>Consider a sequence of random variables $X_1,X_2,\cdots,X_n$. Each random variable can take any value in a finite set $\mathcal V$. For now we assume the length $n$ is fixed (e.g., n = 100).</p>
<h4 id="First-Order-Markov-Process"><a href="#First-Order-Markov-Process" class="headerlink" title="First-Order Markov Process"></a>First-Order Markov Process</h4><p>$$<br>\begin{align}<br>&amp;P(X_1 = x_1,X_2 = x_2,\cdots,X_n = x_n)\\<br>=&amp;P(X_1=x_1)\prod^n_{i=2}P(X_i=x_i|X_1=x_1,\cdots,X_{i-1}=x_{i-1})\\<br>=&amp;P(X_1=x_1)\prod^n_{i=2}P(X_i=x_i|X_{i-1}=x_{i-1})<br>\end{align}\\<br>\text{The first-order Markov assumption: For any }i\in\{2,\cdots, n\}\text{, for any } x_1,\cdots,x_n,\\<br>P(X_i=x_i|X_1=x_1,\cdots,X_{i-1}=x_{i-1})=P(X_i=x_i|X_{i-1}=x_{i-1})<br>$$</p>
<h4 id="Second-Order-Markov-Process"><a href="#Second-Order-Markov-Process" class="headerlink" title="Second-Order Markov Process"></a>Second-Order Markov Process</h4><p>$$<br>\begin{align}<br>&amp;P(X_1 = x_1,X_2 = x_2,\cdots,X_n = x_n)\\<br>=&amp;P(X_1=x_1)P(X_2=x_2|X_1=x_1)\prod^n_{i=3}P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})\\<br>=&amp;\prod^n_{i=1}P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})<br>\end{align}<br>\\<br>\text{For convenience we assume } x_0=x_{-1}=\text{_, where _ is a special “start” symbol.}\\<br>\text{(assumption: }P(X_i=x_i|X_1=x_1,\cdots,X_{i-1}=x_{i-1})=P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})\text{)}<br>$$</p>
<p>We would like the length of the sequence, $n$, to be a random variable. A simple solution is to always define $X_n $= STOP, where STOP is a special symbol. The intuition is at each point, we’re generating some symbol, $X_i,$ and if we ever generate STOP, we immediately terminate the process.</p>
<h3 id="Trigram-Language-models"><a href="#Trigram-Language-models" class="headerlink" title="Trigram Language models"></a>Trigram Language models</h3><p>Each sequence of three words is called a trigram. A trigram language model consists of:</p>
<ol>
<li>a finite set $\mathcal V$</li>
<li>a set of parameters $q(w|u,v)$ for each trigram $u,v,w$ such that $w\in\mathcal V\cup\{\text{STOP}\}$, and $u,v\in\mathcal V\cup\{\text*\}$ </li>
</ol>
<p>The model treats sentences as being generated by a second-order Markov process.  For any sentence $x_1\cdots x_n$ where $x_i\in\mathcal V$ for $i = 1,\cdots,n-1$, and $x_n=$STOP, the probability of the sentence under the trigram language model is<br>$$<br>p(x_1\cdots x_n)=\prod^n_{i=1}q(x_i|x_{i-2,}x_{i-1}),\text{ where we define }x_0=x_{-1}=\text_.<br>$$<br>For the sentence $\text{the dog barks STOP}$, we would have<br>$$<br>p(\text{the dog barks STOP}) = q(\text{the}|\text{_, _})\times q(\text{dog}|\text{_,the})\times q(\text{barks}|\text{the, dog})\times q(\text{STOP}|\text{dog, barks})<br>$$<br>Now we need to estimate the parameter $q$, a natural estimate (maximum likelihood estimate) is<br>$$<br>q(w_i|w_{i-2},w_{i-1})=\frac{\text{Count}(w_{i-2},w_{i-1},w_i)}{\text{Count}(w_{i-2},w_{i-1})}<br>$$<br><strong>Sparse Data Problem</strong>: We have $N^3$ parameters in the model. If $N=20,000$, there would be $8\times10^{12}$ parameters. This is a very large number, in comparison to the number of training examples that we have. In many cases, the counts $\text{Count}(w_{i-2},w_{i-1},w_i)$ on the numerator may be equal to 0. Because we simply haven’t seen this particular triagram in training data, in which case the estimate will be equal to 0. That’s problematic because there are so many trigrams possible. Seeing the trigram zero times in training doesn’t mean that we should say the estimate is equal to 0. Still, in some cases, the denominator $\text{Count}(w_{i-2},w_{i-1})$ may be zero, in which case the ratio is completely undefined and the estimate really falls apart.</p>
<h3 id="Perplexity"><a href="#Perplexity" class="headerlink" title="Perplexity"></a>Perplexity</h3><p>We have some test data, $m$ sentences $s_1,s_2,s_3,\cdots,s_m$. We could look at the probability under our model $\prod_{i=1}^m p(s_i)$. Or more conveniently, the log probability<br>$$<br>\log\prod_{i=1}^m p(s_i)=\sum_{i=1}^m\log p(s_i）<br>$$<br>The higher the log probability, the better our language model is. In fact the usual evaluation measure is perplexity.<br>$$<br>\text{Perplexity}=2^{-l}\qquad\text{where}\qquad l=\frac{1}{M}\sum^m_{i=1}\log p(s_i)\\<br>\text{and M is the total number of words in the test data.}<br>$$<br>Perplexity is a measure of effective “branching factor”. If we assume $q(w|u,v)=1/N$, then $l=\log (1/N)$, and $\text{Perplexity}=N$. It describes the extent of how many different branches each point in the sentence can have in average. Under good models, the “next word” should be well determined and thus there are less branches.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://zccz14.com/images/avatar.png"
               alt="dada" />
          <p class="site-author-name" itemprop="name">dada</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">19</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Captain-X/" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://zccz14.com/" title="zccz14" target="_blank">zccz14</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">dada</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
