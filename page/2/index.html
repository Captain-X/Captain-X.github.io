<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="Vinci">
<meta property="og:url" content="https://Captain-X.github.io/page/2/index.html">
<meta property="og:site_name" content="Vinci">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Vinci">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="https://Captain-X.github.io/page/2/"/>

  <title> Vinci </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Vinci</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp1-3/" itemprop="url">
                  Parameter Estimation in Language Models
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-23T05:27:28-05:00" content="2018-01-23">
              2018-01-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Linear-Interpolation"><a href="#Linear-Interpolation" class="headerlink" title="Linear Interpolation"></a>Linear Interpolation</h3><p>Trigram maximum likehood estimate has relatively low bias but needs very large dataset to accurately estimate.<br>$$<br>q_{ML}(w_i|w_{i-2},w_{i-1})=\frac{\text{Count}(w_{i-2},w_{i-1},w_i)}{\text{Count}(w_{i-2},w_{i-1})}<br>$$<br>Linear interpolation comes up with an estimate that takes into account the maximum likelihood estimates at the trigram, bigram and unigram levels, and brings 3 additional parameters $\lambda_1, \lambda_2, \lambda_3$.<br>$$<br>q(w_i|w_{i-2},w_{i-1})=\lambda_1\times q_{ML}(w_i|w_{i-2},w_{i-1})+\lambda_2\times q_{ML}(w_i|w_{i-1})+\lambda_3\times q_{ML}(w_i)\\<br>\text{where }\lambda_1+\lambda_2+ \lambda_3=1, \text{and }\lambda_i\ge0 \text{ so that}\sum_{w\in\mathcal V’}q(w|u,v)=1\text{ and }q(w|u,v)\ge0<br>$$<br>How to estimate $\lambda$?</p>
<p>We first hold out part of training set as validation data, then define $c’(w_1,w_2,w_3)$ to be the number of times the trigram $(w_1,w_2,w_3)$ is seen in validation set, choose $\lambda_1,\lambda_2,\lambda_3$ to maximize:<br>$$<br>L(\lambda_1,\lambda_2,\lambda_3)=\sum_{(w_1,w_2,w_3)}c’(w_1,w_2,w_3)\log q(w_3|w_1,w_2)<br>$$<br>Allowing $\lambda$ to vary</p>
<p>We take a function $\Pi$ that partitions histories. This means for different ranges of $\text{Count}(w_{i-2},w_{i-1})$, we should find different sets of $\lambda$’s. For example, when $\text{Count}(w_{i-2},w_{i-1})=0$, $\lambda_1^1$should be equal to 0 because the trigram estimate is undefined.<br>$$<br>\Pi(w_{i-2},w_{i-1})=<br>\begin{cases}<br>1, \text{Count}(w_{i-2},w_{i-1})=0\\<br>2,1\le\text{Count}(w_{i-2},w_{i-1})\le2\\<br>3,3\le\text{Count}(w_{i-2},w_{i-1})\le5\\<br>4,\text{otherwise}\\<br>\end{cases}<br>\\<br>q(w_i|w_{i-2},w_{i-1})=\lambda_1^{\displaystyle\Pi(w_{i-2},w_{i-1})} q_{ML}(w_i|w_{i-2},w_{i-1})+\lambda_2^{\displaystyle\Pi(w_{i-2},w_{i-1})} q_{ML}(w_i|w_{i-1})+\lambda_3^{\displaystyle\Pi(w_{i-2},w_{i-1})} q_{ML}(w_i)\\<br>\text{where }\lambda_1^{\displaystyle\Pi(w_{i-2},w_{i-1})}+\lambda_2^{\displaystyle\Pi(w_{i-2},w_{i-1})}+ \lambda_3^{\displaystyle\Pi(w_{i-2},w_{i-1})}=1, \text{and }\lambda_i^{\displaystyle\Pi(w_{i-2},w_{i-1})}\ge0<br>$$</p>
<h3 id="Discounting-methods"><a href="#Discounting-methods" class="headerlink" title="Discounting methods"></a>Discounting methods</h3><h4 id="Discounted-counts"><a href="#Discounted-counts" class="headerlink" title="Discounted counts"></a>Discounted counts</h4><p>Say we’ve seen the following counts:<br><img src="..\img\nlp\counts.png"></p>
<p>The maximum-likelihood estimates are high, particularly for low count items. (They get lucky because the denominator is small and only one appearance will lead to a relatively great value of frequency). Now define “discounted” counts: $\text{Count*}(x) = \text{Count}(x)-0.5$<br><img src="..\img\nlp\discounted.png"></p>
<p>We now have some “missing probability mass”:<br>$$<br>\alpha(w_{i-1})=1-\sum_w\frac{\text{Count*}(w_{i-1},w)}{\text{Count}(w_{i-1})}<br>$$<br>In our example, $\alpha(\text{the})=10\times0.5/48=5/48$</p>
<h4 id="Katz-Back-Off-Models-Bigrams"><a href="#Katz-Back-Off-Models-Bigrams" class="headerlink" title="Katz Back-Off Models (Bigrams)"></a>Katz Back-Off Models (Bigrams)</h4><p>For a bigram model, define two sets<br>$$<br>\mathcal A(w_{i-1})=\{w:\text{Count}(w_{i-1},w)&gt;0\}\\<br>\mathcal B(w_{i-1})=\{w:\text{Count}(w_{i-1},w)=0\}\\<br>$$<br>The bigram estimate is<br>$$<br>q_{BO}(w_i|w_{i-1})=<br>\begin{cases}<br>\displaystyle\frac{\text{Count*}(w_{i-1},w_i)}{\text{Count}(w_{i-1})}, &amp;\text{if }w_i\in\mathcal A(w_{i-1})\\<br>\alpha(w_{i-1})\displaystyle\frac{q_{ML}(w_i)}{\sum_{\displaystyle w\in\mathcal B(w_{i-1})}q_{ML}(w)},&amp; \text{if }w_i\in\mathcal B(w_{i-1})\\<br>\end{cases}<br>$$</p>
<h4 id="Katz-Back-Off-Models-Trigrams"><a href="#Katz-Back-Off-Models-Trigrams" class="headerlink" title="Katz Back-Off Models (Trigrams)"></a>Katz Back-Off Models (Trigrams)</h4><p>For a trigram model, define two sets<br>$$<br>\mathcal A(w_{i-2},w_{i-1})=\{w:\text{Count}(w_{i-2},w_{i-1},w)&gt;0\}\\<br>\mathcal B(w_{i-2},w_{i-1})=\{w:\text{Count}(w_{i-2},w_{i-1},w)=0\}\\<br>$$<br>The trigram estimate is<br>$$<br>q_{BO}(w_i|w_{i-2},w_{i-1})=<br>\begin{cases}<br>\displaystyle\frac{\text{Count*}(w_{i-2},w_{i-1},w_i)}{\text{Count}(w_{i-2},w_{i-1})}, &amp;\text{if }w_i\in\mathcal A(w_{i-2},w_{i-1})\\<br>\alpha(w_{i-2},w_{i-1})\displaystyle\frac{q_{BO}(w_i|w_{i-1})}{\sum_{\displaystyle w\in\mathcal B(w_{i-2},w_{i-1})}q_{BO}(w|w_{i-1})},&amp; \text{if }w_i\in\mathcal B(w_{i-2},w_{i-1})\\<br>\end{cases}<br>$$<br>When assigning the missing probability mass to unseen trigrams, the proportion of each trigram <strong>depends on the bigram BO estimate</strong>. (Iteration)</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>Three steps in deriving the language model probabilities:</p>
<ol>
<li>Expand $p(w_1,w_2,\cdots,w_n)$ using Chain rule.</li>
<li>Make Markov Independence Assumptions $p(w_ i| w_1,\cdots,w_{i-2},w_{i-1}) = p(w_ i|w_{i-2},w_{i-1}) $</li>
<li><strong>Smooth</strong> the estimates using low order counts.</li>
</ol>
<p>Other methods used to improve language models:<br>Conditioning on other more important context information such as “Topic” or “long-range” features<br>Basing on Syntactic models, explicitly trying to incorporate grammatical information. These models can often capture the long range features, which fall outside just a two-way window.</p>
<p>It’s generally hard to improve on trigram models though because they’re simple and very efficient.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp1-2/" itemprop="url">
                  Language Modeling
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-23T03:37:23-05:00" content="2018-01-23">
              2018-01-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Language-modeling-problem"><a href="#Language-modeling-problem" class="headerlink" title="Language modeling problem"></a>Language modeling problem</h3><p>We have some finite vocabulary, which includes all of the words in our language of interest, say $\mathcal V=$ {the, a, man, telescope, Beckham, two, …}</p>
<p>We have an (infinite) set of strings, $\mathcal V^\dagger$. A sentence could have any sequence of words, such as<br>the STOP<br>a STOP<br>the fan STOP<br>the fan saw Beckham STOP<br>the fan saw saw STOP<br>the fan saw Beckham play for Real Madrid STOP<br>STOP</p>
<p><strong>Given a training sample of example sentences, we need to learn a probability distribution.</strong></p>
<p>$p(\text{the STOP})=10^{-12}$<br>$p(\text{the fan STOP})=10^{-8}$<br>$p(\text{the fan saw Beckham STOP})=2\times10^{-8}$<br>$p(\text{the fan saw saw STOP})=10^{-15}$<br>$p(\text{the fan saw Beckham play for Real Madrid STOP})=2\times10^{-9}$</p>
<p>Speech recognition was the original motivation. Related problems are optical character recognition, handwriting recognition. The estimation techniques developed for this problem will be VERY useful for other problems in NLP.</p>
<p>From acoustic point of view, “recognize speech” and “wreck a nice beach” are quite similar. If we simply look at a measure of how compatible this sentence is with the acoustics sentence, it’s quite possible we might confuse these two sentences. In practice, there are many other possibilities which might have a reasonable degree of fit with the acoustic input and might be quite confusable with the true sentence. If we have a language model, we can actually evaluate a probability of each of these sentences. And a language model adds some very useful information to this whole process, which is the fact that the sentence, recognize speech, is probably more probable than the sentence wreck a nice beach. In practice, modern speech recognizers use two sources of information. Firstly, they have some way of evaluating how well each of these sentences match the input from an acoustic point of view. But secondly, they also have a language model which gives a essentially prior probability over the different sentences in the language. It can be very useful in getting rid of this kind of confusions.</p>
<h4 id="A-naive-method"><a href="#A-naive-method" class="headerlink" title="A naive method"></a>A naive method</h4><p>We have $N$ training sentences. For any sentence $x_1\cdots x_n$, $c(x_1\cdots x_n)$ is the number of times the sentence is seen in our training data. A naive estimate is<br>$$<br>p(x1\cdots x_n)=\frac{c(x1\cdots x_n)}{N}<br>$$<br>But it has some very clear deficiencies. Most importantly, it will assign probability 0 to any sentence not seen in our training sample. And we know that we’re continuously seeing new sentences in a language. So, this model has no ability to generalize to new sentences. The most important question is essentially, how can we build models to improve upon this naive estimate and in particular, models which generalize well to new test sentences.</p>
<h3 id="Markov-Processes"><a href="#Markov-Processes" class="headerlink" title="Markov Processes"></a>Markov Processes</h3><p>Consider a sequence of random variables $X_1,X_2,\cdots,X_n$. Each random variable can take any value in a finite set $\mathcal V$. For now we assume the length $n$ is fixed (e.g., n = 100).</p>
<h4 id="First-Order-Markov-Process"><a href="#First-Order-Markov-Process" class="headerlink" title="First-Order Markov Process"></a>First-Order Markov Process</h4><p>$$<br>\begin{align}<br>&amp;P(X_1 = x_1,X_2 = x_2,\cdots,X_n = x_n)\\<br>=&amp;P(X_1=x_1)\prod^n_{i=2}P(X_i=x_i|X_1=x_1,\cdots,X_{i-1}=x_{i-1})\\<br>=&amp;P(X_1=x_1)\prod^n_{i=2}P(X_i=x_i|X_{i-1}=x_{i-1})<br>\end{align}\\<br>\text{The first-order Markov assumption: For any }i\in\{2,\cdots, n\}\text{, for any } x_1,\cdots,x_n,\\<br>P(X_i=x_i|X_1=x_1,\cdots,X_{i-1}=x_{i-1})=P(X_i=x_i|X_{i-1}=x_{i-1})<br>$$</p>
<h4 id="Second-Order-Markov-Process"><a href="#Second-Order-Markov-Process" class="headerlink" title="Second-Order Markov Process"></a>Second-Order Markov Process</h4><p>$$<br>\begin{align}<br>&amp;P(X_1 = x_1,X_2 = x_2,\cdots,X_n = x_n)\\<br>=&amp;P(X_1=x_1)P(X_2=x_2|X_1=x_1)\prod^n_{i=3}P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})\\<br>=&amp;\prod^n_{i=1}P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})<br>\end{align}<br>\\<br>\text{For convenience we assume } x_0=x_{-1}=\text{_, where _ is a special “start” symbol.}\\<br>\text{(assumption: }P(X_i=x_i|X_1=x_1,\cdots,X_{i-1}=x_{i-1})=P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})\text{)}<br>$$</p>
<p>We would like the length of the sequence, $n$, to be a random variable. A simple solution is to always define $X_n $= STOP, where STOP is a special symbol. The intuition is at each point, we’re generating some symbol, $X_i,$ and if we ever generate STOP, we immediately terminate the process.</p>
<h3 id="Trigram-Language-models"><a href="#Trigram-Language-models" class="headerlink" title="Trigram Language models"></a>Trigram Language models</h3><p>Each sequence of three words is called a trigram. A trigram language model consists of:</p>
<ol>
<li>a finite set $\mathcal V$</li>
<li>a set of parameters $q(w|u,v)$ for each trigram $u,v,w$ such that $w\in\mathcal V\cup\{\text{STOP}\}$, and $u,v\in\mathcal V\cup\{\text*\}$ </li>
</ol>
<p>The model treats sentences as being generated by a second-order Markov process.  For any sentence $x_1\cdots x_n$ where $x_i\in\mathcal V$ for $i = 1,\cdots,n-1$, and $x_n=$STOP, the probability of the sentence under the trigram language model is<br>$$<br>p(x_1\cdots x_n)=\prod^n_{i=1}q(x_i|x_{i-2,}x_{i-1}),\text{ where we define }x_0=x_{-1}=\text_.<br>$$<br>For the sentence $\text{the dog barks STOP}$, we would have<br>$$<br>p(\text{the dog barks STOP}) = q(\text{the}|\text{_, _})\times q(\text{dog}|\text{_,the})\times q(\text{barks}|\text{the, dog})\times q(\text{STOP}|\text{dog, barks})<br>$$<br>Now we need to estimate the parameter $q$, a natural estimate (maximum likelihood estimate) is<br>$$<br>q(w_i|w_{i-2},w_{i-1})=\frac{\text{Count}(w_{i-2},w_{i-1},w_i)}{\text{Count}(w_{i-2},w_{i-1})}<br>$$<br><strong>Sparse Data Problem</strong>: We have $N^3$ parameters in the model. If $N=20,000$, there would be $8\times10^{12}$ parameters. This is a very large number, in comparison to the number of training examples that we have. In many cases, the counts $\text{Count}(w_{i-2},w_{i-1},w_i)$ on the numerator may be equal to 0. Because we simply haven’t seen this particular triagram in training data, in which case the estimate will be equal to 0. That’s problematic because there are so many trigrams possible. Seeing the trigram zero times in training doesn’t mean that we should say the estimate is equal to 0. Still, in some cases, the denominator $\text{Count}(w_{i-2},w_{i-1})$ may be zero, in which case the ratio is completely undefined and the estimate really falls apart.</p>
<h3 id="Perplexity"><a href="#Perplexity" class="headerlink" title="Perplexity"></a>Perplexity</h3><p>We have some test data, $m$ sentences $s_1,s_2,s_3,\cdots,s_m$. We could look at the probability under our model $\prod_{i=1}^m p(s_i)$. Or more conveniently, the log probability<br>$$<br>\log\prod_{i=1}^m p(s_i)=\sum_{i=1}^m\log p(s_i）<br>$$<br>The higher the log probability, the better our language model is. In fact the usual evaluation measure is perplexity.<br>$$<br>\text{Perplexity}=2^{-l}\qquad\text{where}\qquad l=\frac{1}{M}\sum^m_{i=1}\log p(s_i)\\<br>\text{and M is the total number of words in the test data.}<br>$$<br>Perplexity is a measure of effective “branching factor”. If we assume $q(w|u,v)=1/N$, then $l=\log (1/N)$, and $\text{Perplexity}=N$. It describes the extent of how many different branches each point in the sentence can have in average. Under good models, the “next word” should be well determined and thus there are less branches.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/nlp1-1/" itemprop="url">
                  Introduction to NLP
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-23T03:33:16-05:00" content="2018-01-23">
              2018-01-23
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="What-is-NLP"><a href="#What-is-NLP" class="headerlink" title="What is NLP"></a>What is NLP</h3><p>Computers using natural language as input and/or output</p>
<div style="align: center"><br><img src="..\img\nlp\concept.png"><br></div>

<h4 id="Key-applications"><a href="#Key-applications" class="headerlink" title="Key applications"></a>Key applications</h4><h5 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h5><p>The problem of mapping sentences in one language to sentences in another.</p>
<h5 id="Information-Extraction"><a href="#Information-Extraction" class="headerlink" title="Information Extraction"></a>Information Extraction</h5><p>The problem in this case is to take some text as input and to produce some structured, basically a database<br>representation of some key content in this text.</p>
<p>Goal: Map a document collection to a structured database</p>
<p>Motivation:</p>
<ul>
<li><p>Complex searches (Find me all the jobs in advertising paying at least $50,000 in Boston)</p>
<p>a search that is very difficult to formulate using a regular search engine</p>
</li>
</ul>
<ul>
<li>Statistical queries (How has the number of jobs in accounting changed over the years?)</li>
</ul>
<h5 id="Text-Summerization"><a href="#Text-Summerization" class="headerlink" title="Text Summerization"></a>Text Summerization</h5><p>The problem in this case is to take a single document or, potentially a group of several documents and to try to condense them down to a summary, which, in some sense, preserves the main information in those documents.</p>
<h5 id="Dialogue-Systems"><a href="#Dialogue-Systems" class="headerlink" title="Dialogue Systems"></a>Dialogue Systems</h5><p>Systems where a human can actually interact with a computer to achieve some task.</p>
<h4 id="Basic-Problems"><a href="#Basic-Problems" class="headerlink" title="Basic Problems"></a>Basic Problems</h4><h5 id="Tagging"><a href="#Tagging" class="headerlink" title="Tagging"></a>Tagging</h5><p>Abstractly, tagging problems take the following form. As input we have some sequence, in this case a sequence of letters and as output we are going to have a tagged sequence where each letter in the input now has an associated tag.</p>
<p>a b e e a f h j $\Rightarrow$ a/C b/D e/C e/C a/D f/C h/D j/C</p>
<ul>
<li><p>Example 1: Part-of-speech tagging</p>
<p>Profits/N soared/V at/P Boeing/N Co./N ,/, easily/ADV topping/V forecasts/N on/P Wall/N Street/N ./.</p>
</li>
<li><p>Example 2: Named Entity Recognition</p>
<p>Profits/NA soared/NA at/NA Boeing/SC Co./CC,/NA easily/NA topping/NA forecasts/NA on/NA Wall/SL Street/CL ./. (SL: the start of a location; CL: the continuation of a location.)</p>
</li>
</ul>
<h5 id="Parsing"><a href="#Parsing" class="headerlink" title="Parsing"></a>Parsing</h5><p>The problem is to map some sentences to some output, which is usually referred to as a parse tree. The parse tree essentially gives some hierarchical decomposition of a sentence, corresponding to its grammatical structure. Once we’ve recovered this kind of representations, we can, for example, recover basic grammatical relations.</p>
<div style="align: center"><br><img src="..\img\nlp\parse_tree.png"><br></div>

<h3 id="Why-is-NLP-hard"><a href="#Why-is-NLP-hard" class="headerlink" title="Why is NLP hard"></a>Why is NLP hard</h3><p>$$<br>\text{At last, a computer that understands you like your mother”}<br>$$</p>
<p>Ambiguity </p>
<ol>
<li>(*) It understands you as well as your mother understands you</li>
<li>It understands (that) you like your mother</li>
<li>It understands you as well as it understands your mother</li>
</ol>
<h4 id="At-the-acoustic-level-speech-recognition"><a href="#At-the-acoustic-level-speech-recognition" class="headerlink" title="At the acoustic level (speech recognition):"></a>At the acoustic level (speech recognition):</h4><ol>
<li>a computer that understands you <strong>like your</strong> mother</li>
<li>a computer that understands you <strong>lie cured</strong> mother</li>
</ol>
<h4 id="At-the-syntactic-level"><a href="#At-the-syntactic-level" class="headerlink" title="At the syntactic level:"></a>At the syntactic level:</h4><div style="align: center"><br><img src="..\img\nlp\syntactic_ambi.png"><br></div>

<p>Different structures lead to different interpretations.</p>
<div style="align: center"><br><img src="..\img\nlp\syntactic_ambi2.png"><br></div>

<h4 id="At-the-semantic-meaning-level"><a href="#At-the-semantic-meaning-level" class="headerlink" title="At the semantic (meaning) level:"></a>At the semantic (meaning) level:</h4><p>Two denitions of mother<br>a woman who has given birth to a child<br>a stringy slimy substance consisting of yeast cells and bacteria; is added to cider or wine to produce vinegar</p>
<p>This is an instance of <strong>word sense ambiguity</strong> (They put money in the bank/ I saw her duck with a telescope)</p>
<h4 id="At-the-discourse-multi-clause-level"><a href="#At-the-discourse-multi-clause-level" class="headerlink" title="At the discourse (multi-clause) level:"></a>At the discourse (multi-clause) level:</h4><p>$\text{Alice says they’ve built a computer that understands you like your mother. But she…}$</p>
<ul>
<li>doesn’t know any details (by far the most plausible interpretation is Alice)</li>
<li>doesn’t understand me at all (your mother)</li>
</ul>
<p>This is an instance of <strong>anaphora</strong>, where she co-referees to some other discourse entity.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/6.2/" itemprop="url">
                  随机模拟计算
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-10T13:11:39-05:00" content="2018-01-10">
              2018-01-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Statistical-Computation-Methods/" itemprop="url" rel="index">
                    <span itemprop="name">Statistical Computation Methods</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="重要抽样方法"><a href="#重要抽样方法" class="headerlink" title="重要抽样方法"></a>重要抽样方法</h3>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/6.1/" itemprop="url">
                  随机数的产生
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-09T01:20:20-05:00" content="2018-01-09">
              2018-01-09
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Statistical-Computation-Methods/" itemprop="url" rel="index">
                    <span itemprop="name">Statistical Computation Methods</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="逆变换法"><a href="#逆变换法" class="headerlink" title="逆变换法"></a>逆变换法</h3><h3 id="合成法"><a href="#合成法" class="headerlink" title="合成法"></a>合成法</h3><h3 id="筛选法"><a href="#筛选法" class="headerlink" title="筛选法"></a>筛选法</h3>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/5.5/" itemprop="url">
                  Bayes分析
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-08T12:03:37-05:00" content="2018-01-08">
              2018-01-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Bayesian-Analysis/" itemprop="url" rel="index">
                    <span itemprop="name">Bayesian Analysis</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Bayes估计"><a href="#Bayes估计" class="headerlink" title="Bayes估计"></a>Bayes估计</h3><h3 id="无信息先验分布"><a href="#无信息先验分布" class="headerlink" title="无信息先验分布"></a>无信息先验分布</h3><h4 id="Bayes假设"><a href="#Bayes假设" class="headerlink" title="Bayes假设"></a>Bayes假设</h4><p>Bayes假设是把$\theta$取值范围上的均匀分布取作$\theta$的先验分布，这是十分自然的想法，但是也会遇到一些麻烦。</p>
<p>一个问题是当$\Theta$为无穷区间时，无法定义一个正常的先验分布，例如对于正态总体$\mathcal N(\theta,1),\theta\in(-\infty,\infty)$，样本容量为1，取先验$\pi(\theta)=c$，有$\int_\Theta\pi(\theta)\text d \theta=\infty$，但是求得的后验分布为$\mathcal N(x,1)$。这种能得到是正常密度函数的后验分布的不正常先验分布称为广义先验分布。此例中的常数$c$的选择实际上不重要，一般取为1，并称这个先验分布为实数集上的均匀分布。</p>
<p>另一个麻烦是Bayes假设不满足变换下的不变性。譬如正态总体中的方差$\sigma^2$和标准差$\sigma$都在$(0,\infty)$上取值。若$\sigma$的先验分布为$\pi(\sigma)=c$，$\eta=\sigma^2$的分布应为$\pi(\sqrt\eta)/(2\sqrt \eta)=2c\sqrt\eta$，那么$\eta=\sigma^2$的无信息先验密度与$\eta^{-\frac{1}{2}}$成比例，这与Bayes假设是矛盾的。这个矛盾说明不能随意设定一个常数为某参数的无信息先验分布，无信息先验分布的选取和参数在总体分布中的地位很有关系。</p>
<h4 id="位置参数的无信息先验"><a href="#位置参数的无信息先验" class="headerlink" title="位置参数的无信息先验"></a>位置参数的无信息先验</h4><p>设总体$X$的密度函数具有形式$p(x-\theta)$，其样本空间和参数空间均为实数集，这类密度函数组成位置参数族，$\theta$称为位置参数。让$X$移动一个常数量得到$Y=X+c$，同时$\eta=\theta+c$，$Y$的密度为$p(y-\eta)$，它仍然是位置参数族的成员，因此</p>
<h4 id="尺度参数的无信息先验"><a href="#尺度参数的无信息先验" class="headerlink" title="尺度参数的无信息先验"></a>尺度参数的无信息先验</h4><p>设总体$X$的密度函数具有形式$\frac{1}{\sigma}p(\frac{x}{\sigma})$，其参数空间为正实数集，这类密度函数组成尺度参数族，$\sigma$称为尺度参数。</p>
<h4 id="Jeffreys先验分布"><a href="#Jeffreys先验分布" class="headerlink" title="Jeffreys先验分布"></a>Jeffreys先验分布</h4><h3 id="多层先验分布"><a href="#多层先验分布" class="headerlink" title="多层先验分布"></a>多层先验分布</h3><p>当所给定先验分布中超参数难于确定时，可以对超参数再给出一个先验，第二个先验称为超先验。</p>
<p>例. </p>
<h3 id="可信域"><a href="#可信域" class="headerlink" title="可信域"></a>可信域</h3>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/5.4/" itemprop="url">
                  Bayes决策准则
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-08T12:03:18-05:00" content="2018-01-08">
              2018-01-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Bayesian-Analysis/" itemprop="url" rel="index">
                    <span itemprop="name">Bayesian Analysis</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="先验分布"><a href="#先验分布" class="headerlink" title="先验分布"></a>先验分布</h3><p>经典统计推断的主要依据是样本信息，来自于“新鲜的”从总体抽取出来的样本，而经验和历史资料也可以用于统计推断和统计决策，它们大多存在于抽样试验之前，称为先验信息。</p>
<p>Bayes学派最基本的观点是：任一未知量$\theta$都可以看作随机变量，可用一个概率分布去描述，这个分布称为先验分布。如今经典学派已不反对这一观点，现在两派争论的焦点是如何利用各种先验信息合理地确定先验分布。先验分布切不可草率定下。</p>
<p>当随机现象不能重复，无法用频率方法确定事件概率时，人们可以根据经验对该事件发生可能性给出个人信念。主观概率至少能使人们在频率方法不适用时也可谈论概率，使用统计方法。</p>
<h3 id="Bayes风险准则"><a href="#Bayes风险准则" class="headerlink" title="Bayes风险准则"></a>Bayes风险准则</h3><p>Bayes风险是按先验分布算出来的平均风险<br>$$<br>R_\pi=\int_\Theta R(\theta,\delta)\pi(\theta)\text d\theta<br>$$<br>若在决策函数类$\mathscr D$中存在$\delta\text_(x)$，使得$R_\pi(\delta\text_)=\inf_{\delta\in \mathscr D}R_\pi(\delta)$，则称$\delta\text*(x)$为决策函数类$\mathscr D$在Bayes风险准则下的最优决策函数，简称为Bayes决策函数或Bayes解。Bayes决策函数是对于固定的先验分布而言的。</p>
<h3 id="Bayes公式"><a href="#Bayes公式" class="headerlink" title="Bayes公式"></a>Bayes公式</h3><p>在经典统计中，依赖于参数$\theta$的密度函数记为$p(x;\theta)$或$p_\theta(x)$，表示不同的$\theta$对应不同的分布。Bayes学派认为密度函数是在随机变量$\theta$给定某个值时，$X$的条件密度函数，故应记为$p(x|\theta)$.从Bayes观点看，样本的产生要分两步进行，首先设想从先验分布$\pi(\theta)$中产生一个观察值$\theta$，然后再从条件分布$p(x|\theta)$产生样本观察值$x=(x_1,\cdots,x_n)$，这时样本$X$的联合条件密度函数为$p(x|\theta)=\prod_{i=1}^n p(x_i|\theta)$，这个联合分布综合了样本信息，称为似然函数。样本$X$与参数$\theta$的联合分布为$h(x,\theta)=p(x|\theta)\pi(\theta)$，把先验信息和样本信息都综合到一起了。为了对$\theta$作出统计决策，可把联合分布进行分解$h(x)=\pi(\theta|x)m(x)$，其中$m(x)=\int_\Theta h(x,\theta)\text d \theta$ $=\int_\Theta p(x|\theta)\pi(\theta)\text d \theta$是$x$的边际密度函数，<strong>它与$\theta$无关，或者说$m(x)$不含$\theta$的任何先验信息</strong>。</p>
<p>$$<br>\pi(\theta|x)=\frac{h(x,\theta)}{m(x)}=\frac{p(x|\theta)\pi(\theta)}{\int_\Theta p(x|\theta)\pi(\theta)\text{d}\theta}<br>$$</p>
<p>这个在样本给定下$\theta$的条件分布称为$\theta$的后验分布，它是在样本给定下集中了样本与先验中有关$\theta$的一s切信息，比先验分布更接近于实际情况。从总体获取样本$X$后，Bayes公式把人们对$\theta$的认识由$\pi(x)$<strong>调正</strong>到$\pi(\theta|x)$。<br>$$<br>\text{先验信息}\oplus\text{样本信息}\Rightarrow\text{后验信息}\\<br>\pi(\theta)\oplus p(x|\theta)\Rightarrow\pi(\theta|x)<br>$$<br>例. $P(A)=\theta$，为估计$\theta$做了$n$次独立观察，其中事件$A$出现的次数$X\sim b(n,\theta)$，似然函数为$p(x|\theta)=\binom{n}{x}\theta^x(1-\theta)^{n-x}$, $x=0,1,\cdots,n$. 假如在试验之前对$A$没有什么了解，说不上$\theta$的大小，建议使用Bayes假设：将$U(0,1)$作为$\theta$的先验分布。<br>$$<br>\pi(\theta)=1,0&lt;\theta&lt;1\\<br>h(x,\theta)=p(x|\theta)\pi(\theta)=\binom{n}{x}\theta^x(1-\theta)^{n-x},x=0,1,\cdots,n,0&lt;\theta&lt;1\\<br>m(x)=\int_0^1h(x,\theta)\text d\theta=\binom{n}{x}\frac{\Gamma(x+1)\Gamma(n-x+1)}{\Gamma(n+2)}\\<br>\pi(\theta|x)=\frac{h(x,\theta)}{m(x)}=\frac{\Gamma(n+2)}{\Gamma(x+1)\Gamma(n-x+1)}\cdot\theta^x(1-\theta)^{n-x},0&lt;\theta&lt;1\\<br>可见，\theta\sim Be(x+1,n-x+1)<br>$$</p>
<div style="align: center"><br><img src="..\img\bayes.png"><br></div>

<p>定理：在有充分统计量（如正态分布中的$\bar X$和$Q$）的场合，后验分布可通过充分统计量的分布求得。</p>
<h3 id="共轭先验分布"><a href="#共轭先验分布" class="headerlink" title="共轭先验分布"></a>共轭先验分布</h3><p>在上例中，先验分布$U(0,1)$实际上是参数为(1,1)的Beta分布，而其后验分布也为Beta分布，这一现象并非偶然，假如把$\theta$的先验分布换成一个一般的Beta分布$Be(a,b)$，其中$a&gt;0,b&gt;0$，经过类似计算后得到后验分布仍然是Beta分布$Be(a+x,b+n-x)$. 在确定总体分布族和先验分布族的情况下，若对任意的总体分布和先验分布，都有后验分布在先验分布族内，称此先验分布族为该总体分布族的共轭分布族（关于某一参数）。共轭先验分布是对某一分布中的参数而言的，离开指定参数和其所在的分布谈论共轭先验分布是没有意义的。下表是一些常用的共轭先验分布。</p>
<table>
<thead>
<tr>
<th style="text-align:center">总体分布</th>
<th style="text-align:center">参数</th>
<th style="text-align:center">共轭先验分布</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">二项分布</td>
<td style="text-align:center">成功概率</td>
<td style="text-align:center">Beta分布</td>
</tr>
<tr>
<td style="text-align:center">泊松分布</td>
<td style="text-align:center">均值</td>
<td style="text-align:center">Gamma分布</td>
</tr>
<tr>
<td style="text-align:center">指数分布</td>
<td style="text-align:center">均值倒数</td>
<td style="text-align:center">Gamma分布</td>
</tr>
<tr>
<td style="text-align:center">正态分布（方差已知）</td>
<td style="text-align:center">均值</td>
<td style="text-align:center">正态分布</td>
</tr>
<tr>
<td style="text-align:center">正态分布（均值已知）</td>
<td style="text-align:center">方差</td>
<td style="text-align:center">倒Gamma分布</td>
</tr>
</tbody>
</table>
<p>例. $X_1,X_2,\cdots,X_n$是来自于正态分布$\mathcal N(\theta,\sigma^2)$的一个样本，其中$\sigma^2$已知，此样本的联合密度函数为<br>$$<br>p(x|\theta)=\left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\theta)^2\right\}<br>$$<br>现取另一正态分布$\mathcal N(\mu,\tau^2)$作为正态均值$\theta$的先验分布，<br>$$<br>\pi(\theta)=\frac{1}{\sqrt {2\pi}\tau}\exp \left\{-\frac{1}{2\tau^2}(\theta-\mu)^2\right\}\\<br>h(x,\theta)=K_1\exp\left\{-\frac{1}{2}[A\theta^2-2B\theta+C]\right\}=K_1\exp\left\{-\frac{(\theta-B/A)^2}{2/A}-\frac{1}{2}(C-B^2/A)\right\}\\<br>其中K_1=(2\pi)^{-(n+1)/2}\tau^{-1}\sigma^{-n},\sigma_0^=\frac{\sigma^2}{n},A=\frac{1}{\sigma_0^2}+\frac{1}{\tau^2},B=\frac{\bar x}{\sigma_0^2}+\frac{\mu}{\tau^2},C=\frac{\sum_{i=1}^n x_i^2}{\sigma^2}+\frac{\mu^2}{\tau^2}\\<br>m(x)=\int_{-\infty}^\infty h(x,\theta)\text d \theta=K_1\exp\left\{-\frac{1}{2}(C-B^2/A)\right\}\sqrt\frac{2\pi}{A}\\<br>\pi(\theta|x)=\sqrt\frac{A}{2\pi}\exp\left\{-\frac{(\theta-B/A)^2}{2/A}\right\}<br>$$<br>后验分布为$\mathcal N(\frac{B}{A},\frac{1}{A})$，均值$\displaystyle \mu_1=\frac{B}{A}=\frac{\bar x\sigma_0^{-2}+\mu\tau^{-2}}{\sigma_0^{-2}+\tau^{-2}}$，方差$\displaystyle \tau_1^2=\frac{1}{A}=(\sigma_0^{-2}+\tau^{-2})^{-1}$</p>
<p>可以发现后验均值是先验均值和样本均值的加权平均$\mu_1=\lambda\mu+(1-\lambda)\bar x,\lambda=\tau^{-2}/(\sigma_0^{-2}+\tau^{-2})$，先验方差越小（先验分布越集中，对先验均值越确信），先验均值在后验均值中的比重越大，相反，若先验方差越大则先验均值的比重越小，当先验方差趋于无穷（先验分布为均匀分布）时，后验均值等于样本均值，这可以描述为采用无信息先验分布时，估计只利用了样本信息，实际上就等于通过样本算得的MLE或矩估计值。另一方面，当样本量$n$越大时，$\sigma_0^2=\sigma^2/n$越小，从而样本均值在后验均值中的比重越大，特别当$n$无限增大时，先验均值在后验均值中已是微不足道。</p>
<h3 id="核"><a href="#核" class="headerlink" title="核"></a>核</h3><p>正态分布的密度函数可以表示为$p(x)\propto\exp\{-(x-\mu)^2/2\sigma^2\}$，正比于符号两侧只相差一个不依赖于密度函数的主元$x$的因子，利用分布的正则性，不难恢复这个因子。熟悉分布的核可以简化后验分布的计算，后验分布$\pi(\theta|x)$的核一定是$\theta$的函数，Bayes公式中的分母$m(x)$是与$\theta$无关的因子，可以略去，从而有$\pi(\theta|x)\propto p(x|\theta)\pi(\theta)$，还可以进一步用$p(x|\theta)$和$\pi(\theta)$的核代入简化运算。如在上例中：<br>$$<br>\begin{align}<br>\pi(\theta|x)&amp; \propto p(x|\theta)\pi(\theta)\\<br>&amp;\propto \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\theta)^2\right\}\exp \left\{-\frac{1}{2\tau^2}(\theta-\mu)^2\right\}\\<br>&amp;\propto \exp\{-\frac{1}{2}[A\theta^2-2B\theta]\}\\<br>&amp;\propto \exp\{-\frac{A}{2}(\theta-B/A)^2\}<br>\end{align}<br>$$<br>可以看出后验分布为$\mathcal N(1/A,B/A)$.</p>
<table>
<thead>
<tr>
<th style="text-align:center">分布</th>
<th style="text-align:center">核</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$b(n,\theta)$</td>
<td style="text-align:center">$\theta^x(1-\theta)^{n-x}$</td>
</tr>
<tr>
<td style="text-align:center">$P(\lambda)$</td>
<td style="text-align:center">$\lambda^xe^{-\lambda}$</td>
</tr>
<tr>
<td style="text-align:center">$Be(a,b)$</td>
<td style="text-align:center">$x^{a-1}(1-x)^{b-1}$</td>
</tr>
<tr>
<td style="text-align:center">$Ga(a,\lambda)$</td>
<td style="text-align:center">$x^{a-1}e^{-\lambda x}$</td>
</tr>
<tr>
<td style="text-align:center">$N(\mu,\sigma^2)$</td>
<td style="text-align:center">$\exp \{-(x-\mu)^2/2\sigma^2\}$</td>
</tr>
</tbody>
</table>
<p>又例. 二项分布中对成功概率$\theta$的估计。总体$X\sim b(n,\theta)$的核为$\theta^x(1-\theta)^{n-x}$，先验分布$Be(a,b)$的核为$\theta^{a-1}(1-\theta)^{b-1}$，后验分布$\pi(\theta|x)\propto\theta^{a+x-1}(1-\theta)^{b+n-x-1}$，这就是Beta分布$Be(a+x,b+n-x)$的核，均值先验均值和样本均值的加权和<br>$$<br>\frac{a+x}{a+b+n}=\frac{a+b}{a+b+n}\frac{a}{a+b}+\frac{n}{a+b+n}\frac{x}{n}<br>$$<br>先验分布中所含的参数称为超参数，下面以前面的共轭先验分布$Be(a,b)$为例讨论确定超参数的几个途径。</p>
<p>1) 假如根据先验信息能获得成功概率的若干个观察值（一般是从历史数据整理加工获得的），可以通过矩估计法来确定两个参数。<br>$$<br>\begin{cases}<br>\displaystyle \frac{a}{a+b}=\bar \theta=\frac{1}{n}\sum_{i=1}^n\theta_i\\<br>\displaystyle\frac{ab}{(a+b)^2(a+b+1)}=S_\theta^2=\frac{1}{n-1}\sum_{i=1}^n(\theta_i-\bar \theta)^2<br>\end{cases}<br>$$<br>（这个地方的理解，在先验信息关键参数都未知的情况下，历史样本（还是可以理解为样本）用作确定先验信息而非当作样本信息。因为先验信息实在有限，看上去没有必要用Bayes估计。但是倘若用这些样本直接来估计未知量，只能得到一个估计值。若以Bayes学派的观点，未知量都应该看作随机变量，用概率分布描述，而利用先验信息中分布为Beta分布的信息用Bayes估计可以得到未知量的一个分布，更符合Bayes学派对未知量的理解。）</p>
<p>2) 假如根据先验信息只能获得先验均值$\bar \theta$，令$\frac{a}{a+b}=\bar \theta$，一个方程无法确定两个参数，但是因为方差随$a+b$增大而减小，可以根据对先验均值的确信程度来确定$a+b$的值，如果很确信，则$a+b$可以取得大一点。</p>
<p>3) 还可通过两个分位数利用数值积分求解$a,b$.</p>
<p>4) 如果决策者对成功概率一无所知，则可使用Bayes假设，用$Be(1,1)$作为先验分布。</p>
<h3 id="后验风险准则"><a href="#后验风险准则" class="headerlink" title="后验风险准则"></a>后验风险准则</h3><p>$\mathscr D$中任一个决策函数$\delta(x)$的损失函数对后验分布的数学期望称为后验风险<br>$$<br>R_\pi(\delta|x)=E_{\theta|x}[L(\theta,\delta(x))]=\int_\Theta L(\theta,\delta(x))\pi(\theta|x)\text d \theta<br>$$<br>$\delta\text*(x)$为该统计决策问题在后验风险准则下的最优决策函数，或者Bayes（后验型）决策函数，假如$R_\pi(\delta\text*|x)=\inf_{\delta\in\mathscr D}R_\pi(\delta|x)$.</p>
<p>例. $X=(X_1,\cdots,X_n)$是来自正态总体$\mathcal N(\mu,1)$的一个样本，假如参数$\mu$的先验分布取为共轭先验分布$\mathcal N(0,\tau^2)$，其中$\tau$已知，损失函数取为0-1损失函数，求参数$\mu$的Bayes（后验型）估计。<br>$$<br>L(\mu,\delta)=<br>\begin{cases}<br>0,|\delta-\mu|\le\epsilon\\<br>1,|\delta-\mu|&gt;\epsilon<br>\end{cases}<br>$$</p>
<p>$$<br>\begin{align}<br>\pi(\mu|x)&amp;\propto p(x|\mu)\pi(\mu)\\<br>&amp; \propto \exp\left\{-\frac{1}{2}[\sum_{i=1}^n (x_i-\mu)^2+\frac{\mu ^2}{\tau^2}]\right\}\\<br>&amp; \propto \exp\left\{-\frac{1}{2} (n+\tau^{-2})\mu^2+\sum_{i=1}^n x_i\mu\right\}\\<br>&amp; \propto \exp\left\{-\frac{1}{2}(n+\tau^{-2})(\mu-\frac{\sum_{i=1}^nx_i}{n+\tau^{-2}})^2\right\}<br>\end{align}\\<br>\quad\\<br>\mu\sim\mathcal N(\frac{\sum_{i=1}^nx_i}{n+\tau^{-2}},\frac{1}{n+\tau^{-2}})\\<br>R_\pi(\delta|x)=1-P(|\delta-\mu|\le\epsilon)<br>$$</p>
<p>要在定长$2\epsilon$区间上概率最大，$\delta(x)$只能取后验分布的均值，即在此场合下，$\mu$的Bayes后验型估计为<br>$$<br>\delta_\tau(x)=\frac{\sum_{i=1}^n x_i}{n+\tau^{-2}}<br>$$<br>它与经典方法得到的估计$\bar X$是不同的，一般有$\delta_\tau(x)&lt;\bar X$，$\delta_\tau(x)$比$\bar X$更接近0，这是先验分布给我们带来一些信息之故（先验分布均值为0）. 只有当$\tau\rightarrow\infty$时，才有$\delta_\tau(x)\rightarrow\bar X$（相当于先验为Bayes假设，没有先验信息）.</p>
<p>对给定的统计决策问题（包括先验分布的给定）和决策函数类$\mathscr D$，当Bayes风险存在有限取值时，<strong>Bayes决策函数$\delta\text{_}(x)$和Bayes（后验型）决策函数$\delta\text{_}\text{*}(x)$是等价的。</strong></p>
<p>证明：<br>$$<br>R_\pi(\delta)=\int_\Theta R(\theta,\delta)\pi(\theta)\text d\theta=\int_\Theta\left\{\int_\mathscr X L(\theta,\delta)p(x|\theta)\text dx\right\}\pi(\theta)\text d\theta\\<br>R_\pi(\delta)=\int_\mathscr X\left\{\int_\Theta L(\theta,\delta)\pi(\theta|x)\text d\theta\right\}m(x)\text dx=\int_\mathscr X R_\pi(\delta|x)m(x)\text dx\\<br>\text{这表明Bayes风险是后验风险对边际分布的数学期望}\\<br>\begin{align}<br>R_\pi(\delta\text_)=\inf_{\delta\in\mathscr D}R_\pi(\delta)&amp;=\inf_{\delta\in\mathscr D}\int_\mathscr X R_\pi(\delta|x)m(x)\text dx\\<br>&amp;\ge\int_\mathscr X \inf_{\delta\in\mathscr D}R_\pi(\delta|x)m(x)\text dx\\<br>&amp;=\int_\mathscr X R_\pi(\delta\text_\text_|x)m(x)\text dx\\<br>&amp;=R_\pi(\delta\text_\text_)\\<br>又有R_\pi(\delta\text_\text_)&amp;\ge\inf_{\delta\in\mathscr D}R_\pi(\delta)=R_\pi(\delta\text_)\\<br>故R_\pi(\delta\text_\text_)&amp;=R_\pi(\delta\text_)\\<br>\end{align}\\<br>这表明使后验风险最小的决策函数\delta\text_\text_同时也使\text{Bayes}风险最小\\<br>另外，由R_\pi(\delta\text_\text_)=R_\pi(\delta\text_)可得\int_\mathscr X R_\pi(\delta\text_|x)m(x)\text dx=\int_\mathscr X R_\pi(\delta\text_\text_|x)m(x)\text dx\\<br>又R_\pi(\delta\text_\text_|x)= \inf_{\delta\in\mathscr D}R_\pi(\delta|x)\le R_\pi(\delta\text_|x)\\<br>所以R_\pi(\delta\text*|x)=R_\pi(\delta\text_\text_|x)\\<br>这表明\text{Bayes}风险最小的决策函数\delta\text*同时也使后验风险最小<br>$$<br>人们更常使用后验风险途径来寻找Bayes决策函数，因为它的计算相对简单和方便。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/5.3/" itemprop="url">
                  决策函数的容许性
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-08T10:51:31-05:00" content="2018-01-08">
              2018-01-08
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Bayesian-Analysis/" itemprop="url" rel="index">
                    <span itemprop="name">Bayesian Analysis</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="容许性"><a href="#容许性" class="headerlink" title="容许性"></a>容许性</h3><p>利用风险函数对决策函数的容许性进行评价不在于选优，而在于除劣，实际意义在于缩小挑选的范围。</p>
<p>$\delta_0(D|x)$称为非容许的，如果$\bar {\mathscr D}$中存在另一个决策函数$\delta_1(D|x)$对于任意$\theta\in \Theta$风险函数都小于等于$R(\theta,\delta_0)$，而且存在一个状态$\theta_0$下，$R(\theta_0,\delta_1)&lt;R(\theta_0,\delta_0)$，（风险一致更小）。</p>
<h3 id="Stein效应"><a href="#Stein效应" class="headerlink" title="Stein效应"></a>Stein效应</h3><p>正态均值用其样本均值去估计有很好的性质，它是无偏的，方差最小的和有效的。但是当把这样的估计推广到$p$元正态分布场合时出现了问题。在二次损失函数下，$p\ge3$时，样本均值向量是正态均值向量的非容许估计。<br>$$<br>\boldsymbol\delta^{JS}(\boldsymbol X)=\left(1-{p-2\over \boldsymbol X’\boldsymbol X}\right)\boldsymbol X<br>$$</p>
<h3 id="单参数指数族中的容许性问题"><a href="#单参数指数族中的容许性问题" class="headerlink" title="单参数指数族中的容许性问题"></a>单参数指数族中的容许性问题</h3><h3 id="最小最大估计的容许性"><a href="#最小最大估计的容许性" class="headerlink" title="最小最大估计的容许性"></a>最小最大估计的容许性</h3><p>在一个统计决策问题中，假如$\delta_0(x)$是参数$\theta$的唯一最小最大估计，则$\delta_0(x)$也是参数$\theta$的容许估计。假如$\delta_0(x)$是参数$\theta$的容许估计，且在参数空间$\Theta$上有常数风险，则$\delta_0(x)$也是参数$\theta$的最小最大估计。（均用反证法证）</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/5.1_2/" itemprop="url">
                  统计决策理论
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2018-01-04T03:42:35-05:00" content="2018-01-04">
              2018-01-04
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Bayesian-Analysis/" itemprop="url" rel="index">
                    <span itemprop="name">Bayesian Analysis</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>决策问题的三个要素：状态集$\Theta=\{\theta\}$，行动集$\Delta=\{a\}$和收益函数$Q=(\theta,a)$，为了更好地做出决策，可以利用先验信息和样本信息。利用样本信息的决策问题称为统计决策问题，如果还利用先验信息，就称为Bayes决策问题。统计决策问题的三个基本要素是可控参数统计结构，行动空间和损失函数。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>$$<br>L(\theta,a)=\lambda(\theta)\cdot g(|a-\theta|)<br>$$</p>
<p>其中$\lambda(\theta)&gt;0$，且有限，它反映决策中，由于$\theta$的不同，即使同一个偏差$|a-\theta|$造成的危害性常常不一样。$g$是非降函数。</p>
<ol>
<li><p>加权平方损失函数<br>$$<br>L(\theta,a)=\lambda(\theta)\cdot (a-\theta)^2<br>$$<br>常选用平方损失函数的原因是二次函数是$g$在零点展开成Taylor级数的近似，也因为与最小二乘法形式相似，便于计算，但是失真之处在于不是上有界，致使损失随偏差按平方率增大直至无穷。</p>
</li>
<li><p>线性损失函数<br>$$<br>L(\theta,a) =<br>\begin{cases}<br>K_0(\theta-a),  &amp; a\le\theta\\<br>K_1(a-\theta), &amp; a&gt;\theta<br>\end{cases}<br>$$<br>$K_0, K_1$为常数，反映行动$a$低于或高于状态$\theta$的相对重要性。</p>
</li>
<li><p>0-1损失函数<br>$$<br>L(\theta,a) =<br>\begin{cases}<br>0,  &amp; |a-\theta|\le\epsilon \\<br>1, &amp; |a-\theta|&gt;\epsilon<br>\end{cases}<br>$$<br>常在两行动决策问题中使用。</p>
</li>
<li><p>多元二次损失函数<br>$$<br>L(\boldsymbol{\theta},\boldsymbol{a})=(\boldsymbol{a}-\boldsymbol{\theta})’\boldsymbol{A}(\boldsymbol{a}-\boldsymbol{\theta})<br>$$<br>$\boldsymbol{\theta}’=(\theta_1,\cdots,\theta_p), \boldsymbol{a}’=(a_1,\cdots,a_p)$，$\boldsymbol{A}$为$p\times p$阶正定矩阵。当$\boldsymbol{A}$为对角阵，等于$\text{diag}(w_1,\cdots,w_p)$时，$p$元损失函数为<br>$$<br>L(\boldsymbol{\theta},\boldsymbol{a})=\sum^p_{i=1}w_i(a_i-\theta_i)^2<br>$$</p>
</li>
</ol>
<h3 id="决策函数"><a href="#决策函数" class="headerlink" title="决策函数"></a>决策函数</h3><p>在一个统计决策问题中，从样本空间到行动空间的一个可测映照$\delta(x)$成为（非随机化）决策函数，非随机化是指样本一旦确定，所采取的行动也唯一确定，再无任何随机性可言。</p>
<h3 id="风险函数"><a href="#风险函数" class="headerlink" title="风险函数"></a>风险函数</h3><p>风险就是平均损失，是损失函数$L(\theta,\delta(X))$关于样本$X$的分布$p_\theta(x)$的数学期望。<br>$$<br>R(\theta,\delta)=E_{X|\theta}[L(\theta,\delta(X))]=\int_\mathscr X L(\theta,\delta(x))p_\theta(x) \rm d\mu<br>$$</p>
<h3 id="最小最大估计"><a href="#最小最大估计" class="headerlink" title="最小最大估计"></a>最小最大估计</h3><p>在样本空间$\mathscr X$到行动空间$\Delta$上的一切决策函数构成的决策函数类$\mathscr D$中寻找一致最优决策函数往往不能奏效，只能在其中一个特定子类$\mathscr D_1\subset \mathscr D$才有可能找到一致最优决策函数，或者扩大到随机化决策函数类中才能寻得一致最优决策函数。假如我们不是全面对风险函数进行逐点比较，而只对风险函数某一侧面进行比较，选出在这一侧面上最优的决策函数，这就形成了众多的统计决策准则，其中尤以最小最大准则和Bayes风险准则最常用和最有意义。</p>
<p>Minimax准则是对$\mathscr D$中每个决策函数算出其最大风险值，再在所有最大风险值中选取相对最小值。它是预防最大风险值出现的一种稳妥保守的策略（e.g. 买保险）。<br>$$<br>\sup_{\theta\in\Theta} R(\theta,\delta\text{*}(x))=\inf_{\delta(x)\in\mathscr D} \sup_{\theta\in\Theta}R(\theta,\delta(x))<br>$$<br>例. 设$X$是正态总体$\mathcal N(\theta,1)$抽取的容量为1的样本，寻找$\theta$的估计。若取平方损失，则$\mathscr D_1=\{\delta_c:\delta_c(X)=cX,c\in R\}$中任一个估计的风险函数为<br>$$<br>R(\theta,\delta_c)=E_{x|\theta}(cX-\theta)^2=E_{x|\theta}[c(X-\theta)+(c-1)\theta]^2=c^2+(c-1)^2\theta^2<br>$$<br>当$c\ge1$时，$\delta_1(X)=X$是最优，但当$c&lt;1$时，诸风险函数呈交叉状态，故对诸风险函数逐点比较在$\mathscr D_1$中没有一致最小风险估计，但它们的最大风险可以算出<br>$$<br>\sup_{\theta\in R}R(\theta,\delta_c)=\sup_{\theta\in R}[c^2+(c-1)^2\theta^2]=<br>\begin{cases}<br>1, &amp; c=1\\<br>\infty , &amp; c\ne 1<br>\end{cases}<br>$$<br>按最小最大准则，$\delta_1(X)=X$是$\theta$在$\mathscr D_1$中的最小最大估计，其最小最大风险为1。</p>
<h3 id="随机化决策函数"><a href="#随机化决策函数" class="headerlink" title="随机化决策函数"></a>随机化决策函数</h3><p>有时即使样本的观察值$x$给定，还有必要以某种随机方式在$\Delta$中选取决策行动才能改善决策效果。若把非随机决策函数$\delta(x)$看作一次抽样，随机化决策函数$\delta(D|x )$可看作二次抽样：先在样本空间$\mathscr X$中按照总体分布$p_\theta(x)$随机抽取一个样本$x_0$，与此同时就确定了一个条件分布$\delta(D|x_0 )$；再在行动空间中按条件分布$\delta(D|x_0 )$抽取一个行动，用此行动对自然界的状态或参数做出判断。随机化决策函数在实际中很少被采用，因为把行动的最后决策留给某种随机化方法来决定对于当今的决策者是难以接受的，但是在遇到聪明的对手时，随机化决策是最好的方法。</p>
<p>按随机化决策函数确定的行动具有二重随机性，要作二次平均才能确定损失函数和风险函数。<br>$$<br>\bar L(\theta,\delta)=E_{a|x}[L(\theta,a)]=\int_\Delta L(\theta,a)\delta(\text{d} a|x)\\<br>R(\theta,\delta)=E_{x|\theta}\{E_{a|x}[L(\theta,a)]\}=\int_\mathscr X\left\{\int_\Delta L(\theta,a)\delta(\text{d} a|x)\right\}p_\theta(x)\text{d}x<br>$$<br>例. $X\sim P(\lambda),$其中$\lambda\in\{1,2\}.$ 根据一次试验结果 $x$，构造两个非随机化决策函数<br>$$<br>\delta_1(x)=<br>\begin{cases}<br>\lambda_1 &amp;x=0\\<br>\lambda_2 &amp; x=1,2,\cdots<br>\end{cases}\quad<br>\delta_2(x)=<br>\begin{cases}<br>\lambda_1 &amp;x=0,1\\<br>\lambda_2 &amp; x=2,3,\cdots<br>\end{cases}\\<br>\text{取损失矩阵如下}<br>L=<br>\begin{bmatrix}<br>0&amp;50\\<br>100&amp;0<br>\end{bmatrix}<br>$$<br>$R(\lambda_1,\delta_1)=50\cdot P_{\lambda_1}(x&gt;0)=50\times0.6321=31.61,R(\lambda_2,\delta_1)=100\cdot P_{\lambda_2}(x=0)=100\times0.1353=13.53$</p>
<p>$R(\lambda_1,\delta_2)=50\cdot P_{\lambda_1}(x&gt;1)=50\times0.2642=13.21,R(\lambda_2,\delta_2)=100\cdot P_{\lambda_2}(x\le1)=100\times0.4060=40.60$</p>
<p>每个估计量$\delta_j$在$\lambda_1$和$\lambda_2$各有一个风险值，记为$y_1$和$y_2$，把$y_1$和$y_2$看作平面直角坐标系的两个坐标，那么任一估计量就对应平面第一象限中的一点。我们把随机化估计增加到估计类$\mathscr D_1$中去，先考虑一种特殊形式$\delta\text_$，它在样本$x$给定条件下以概率$p$取$\delta_1(x)$，以概率$1-p$取$\delta_2(x)$，所以有$\delta\text_(0)=\lambda_1$，$\delta\text_(2)=\delta\text_(3)=\cdots=\lambda_2$，而$\delta\text_(1)$是一个随机变量，以概率$1-p$取$\lambda_1$，以概率$p$取$\lambda_2$.<br>$$<br>\begin{align}<br>R(\lambda_i,\delta\text_) &amp;=\sum_x\bar L(\lambda_i,\delta\text*(x))P_{\lambda_i}(x)\\<br>&amp;=\sum_x[ L(\lambda_i,\delta_1(x))p+L(\lambda_i,\delta_2(x))(1-p)]P_{\lambda_i}(x)\\<br>&amp;=\sum_x L(\lambda_i,\delta_1)P_{\lambda_i}(x)p+\sum_x L(\lambda_i,\delta_2)P_{\lambda_i}(x)(1-p)\\<br>&amp;=R(\lambda_i,\delta_1)p+R(\lambda_i,\delta_2)(1-p)<br>\end{align}<br>$$<br>由此看出，这种随机化估计量$\delta\text*(x)$对应的点是在$\delta_1(x)$和$\delta_2(x)$的连线上。直线$y_1=y_2$与这条连线的交点即是此决策函数类中的最小最大估计，最小最大风险为24.28，优于原非随机化决策函数。<br>$$<br>31.6p+13.5(1-p)=13.2p+40.6(1-p)\\<br>p=0.6<br>$$<br>这个估计仅在$x=1$时是随机的，其他样本条件下都是非随机的。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/Euler's_theorem/" itemprop="url">
                  Euler's theorem
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-09-19T06:30:54-04:00" content="2016-09-19">
              2016-09-19
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>Euler’s theorem.</strong> Let P be a polyhedron which satifies: (a) Any two vertices of P can be connected by a chain of edges; (b) Any loop on P which is made up of straight line segments (not necessarily edges) seperates P into two pieces. Then v-e+f=2 for P.</p>
<p><strong>Outline proof</strong> </p>
<p>The set of all vertices and edges of P is a connected graph, from which we can easily obtain a subgraph which is a tree and which contains all the vertices of the original, denoted as $T$. Now form a ‘dual’ to $T$. For each face $A$ of $P$ we give $\Gamma$ a vertex $\hat A$. Two vertices $\hat A$ and $\hat B$  of $\Gamma$ are joined by an edge iff the corresponding faces $A$ and $B$ of $P$ are adjacent with intersection an edge that is not in $T$. $\Gamma$ is connected for if two vertices of $\Gamma$ cannot be connected by a chain of edges, then they must be seperated by a loop of $T$. In fact $\Gamma$ is a tree. For if there were a loop  it would seperate $P$ into two distinct pieces, and each of these pieces must contain at least one vertex of $T$. Any attempts to connect two vertices of $T$ which lie in different pieces results in a chain which meets this seperating loop, and therefore in a chain which cannot lie entirely in $T$. This contradicts the fact that $T$ is connected.<br>$$<br>v(T)-e(T)=1,v(\Gamma)-e(\Gamma)=1\\<br>v(T)-[e(T)+e(\Gamma)]+v(\Gamma)=2\\<br>\text{where by construction }v(T)=v, e(T)+e(\Gamma)=e,v(\Gamma)=f<br>$$<br><strong>Topological equivalence</strong></p>
<div align="center"><img src="thicken.jpg" alt=""></div>


          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://zccz14.com/images/avatar.png"
               alt="dada" />
          <p class="site-author-name" itemprop="name">dada</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">21</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/Captain-X/" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://zccz14.com/" title="zccz14" target="_blank">zccz14</a>
                </li>
              
            </ul>
          </div>
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">dada</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
